{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"], "fields": {"title": {"boost": 1000.0}, "text": {"boost": 1.0}, "tags": {"boost": 1000000.0}}}, "docs": [{"location": "Clarity/", "title": "Clarity", "text": ""}, {"location": "Clarity/#authors", "title": "Authors", "text": "<ul> <li>Kevin Moran, George Mason University</li> <li>Ali Yachnes, William &amp; Mary</li> <li>George Purnell, William &amp; Mary</li> <li>Junayed Mahmud, George Mason University</li> <li>Michele Tufano, Microsoft</li> <li>Carlos Bernal Cardenas, Microsoft</li> <li>Denys Poshyvanyk, William &amp; Mary</li> <li>Zach H\u2019Doubler, William &amp; Mary</li> </ul>"}, {"location": "Clarity/#introduction", "title": "Introduction", "text": "<p>Graphical User Interface (GUI) based applications predominate user-facing software in the modern era of computing. High quality applications with well-designed GUIs allow users to instinctively understand underlying features. Thus, intuitively, certain functional properties of applications are encoded into the visual, pixel-based representation of the GUI such that cognitive human processes can determine the computing tasks provided by the interface. This suggests that there are certain latent patterns that exist within visual GUI data which indicate the presence of natural use cases capturing core program functionality. This functionality can be easily interpreted and communicated in natural language (NL) by users after simply glancing at a GUI. </p> <p>Given the inherent representational power of GUIs in conveying program related information, we set forth the following hypothesis that serves as the basis for the work undertaken in this project: </p> <p>The representational power of graphical user interfaces to convey program-related information can be meaningfully leveraged to support and automate software documentation tasks.</p> <p>We are interested in understanding the potential of rich GUI-based representations to support tasks related to automated documentation; a key area of research related to several open problems that sit at the intersection of software documentation, design, and testing. We are interested in exploring the above hypothesis due to the potential impact that advancements in GUI-centric program comprehension could have on important SE tasks.</p> <p>To investigate the potential of automated GUI-centric software documentation, we offer one of the first comprehensive empirical investigations into this new research direction's most fundamental task: generating a natural language description given a screenshot (or screen-related information). To accomplish this, we collect and analyze what is, to the best of the authors' knowledge, the largest dataset of functional descriptions of software GUIs. Furthermore, to investigate the representational power of GUIs to encode information related to functional properties of software, we train and test four Deep Learning (DL) approaches for image captioning, three that learn from image data and one that learns from textual GUI metadata, to predict functional descriptions of software at different granularities. We evaluate the efficacy of these models both quantitatively, by measuring the well accepted BLEU score metric, and qualitatively through a large-scale user study.</p>"}, {"location": "Clarity/#background", "title": "Background", "text": "Fig 1: Generalized overview of multimodal DL architectures for image captioning (with RCNN Region proposals)."}, {"location": "Clarity/#the-connection-between-images-and-nl", "title": "The Connection between Images and NL", "text": "<p>A longstanding research challenge in the field of Artificial Intelligence (AI) and Machine Learning (ML) has been to enable computational programs that are capable of intelligently understanding visual information. Granting computers this ability to comprehend and communicate about images in NL could lead to highly impactful applications, from advancements in intelligent virtual assistants to automated medical diagnoses. In recent years DL-based approaches have proven particularly transformative for several image comprehension tasks. These DL-based techniques have evolved to tackle increasingly more difficult problems. Research has progressed from image classification using large scale datasets like ImageNet, to more advanced tasks including image captioning and dense image captioning. In image classification tasks, a model is trained to evaluate a given image and assign it one of a set of predefined categories. DL-based approaches for this task have surpassed human levels of accuracy. Conversely, image captioning, wherein a model is trained to predict an appropriate sentence to describe an image, are generally still under active research.</p> <p>The task of image captioning is much more difficult than that of classification, as an effective model must be able to both learn salient features from images automatically and semantically equate these features with the proper NL words and grammar that describe them. This task of semantically aligning two completely different modalities of information has led to the development of multimodal DL architectures that jointly embed NL and pixel-based information in order to predict an appropriate description of a given input image. These techniques are typically trained on large-scale datasets that contain images annotated with multiple captions, such as MSCOCO, and have largely drawn inspiration from encoder-decoder neural language models traditionally applied to machine translation tasks. In this paper, we explore three recent architectures for image captioning applied to captioning software screenshots with functional descriptions, neuraltalk2, Google's im2txt framework and show, attend and tell framework, in addition to one neural language model, Google's seq2seq model. We briefly describe these architectures below, starting with the seq2seq model.</p> <p>Google's Neural Machine Translation Model (seq2seq): Several studies have illustrated the successes of encoder-decoder Recurrent Neural Networks (RNNs) for the task of machine translation. The general task defined by machine translation is to transform an input sentence written in one language, to a target translation in another language. Encoder-decoder RNNs typically function by having an ''encoder'' RNN parse a sentence and transform it into a vector-based embedding space. Then, this embedding space is used as input to a ''decoder'' RNN that aims to generate the target sentence. These models are trained on large NL corpora containing both source and target sentences, and the network weights are trained to minimize prediction error. Google's seq2seq framework is a generalized architecture for neural machine translation tasks, and in this paper, we investigate its potential to translate from lexical GUI metadata to NL descriptions of software. </p> <p>Google's Neural Image Captioning Architecture (im2txt): Google's DL model for image captioning builds upon the success of encoder-decoder neural language models. The im2txt framework treats image captioning as a machine translation problem, wherein the source ''sentence'' is an image, and the target ''translation'' is a NL sentence. The generalized architecture of im2txt is shown in Fig. 1. As illustrated, im2txt replaces the encoder RNN with a Convolutional Neural Network (CNN), which have been shown to be highly capable of learning rich image features. Google's implementation uses a Long-Short Term Memory (LSTM) RNN for the ''decoder'' module, which has also proven extremely effective when applied to machine translation tasks. LSTM RNN units are capable of ''remembering'' data values over an arbitrary time period via three gates that regulate information flow. In this architecture, the CNN weights are pre-trained on a large image classification dataset (e.g. ImageNet). </p> <p>Karpathy et.al.\u2019s NeuralTalk2 Architecture: The NeuralTalk2 model architecture proposed by Karpathy et.al. functions along the same general principles as im2txt, with certain key differences. neuraltalk2 shares the goal of predicting a sentence given an input image, and does so with an architecture that jointly embeds both image data and NL sentence data in a shared semantic embedding space. However, unlike im2txt, NeuralTalk2 uses a Region CNN (RCNN) that is capable of detecting and labeling multiple objects within a single image by combining a CNN with a region proposal algorithm. Furthermore, the \"decoder'' component used by Karpathy's architecture is composed of a Bidirectional RNN (BRNN) as opposed to an LSTM. BRNNs function similarly to a standard RNN, however the derived word embeddings are richer data representations thanks to the inclusion of sentence context around a given word (i.e. including context from other words surrounding a given word). Neuraltalk2 is trained in a similar manner to im2txt, where the CNN weights are pre-initialized on a large image classification dataset, and the RCNN is trained end-to-end using the CNN embeddings to predict sentences.</p> <p>Show, attend and Tell: The show, attend and tell model describes the content of an image using attention. This model generates captions using the attention that combines \"hard\" and \"soft\" attention mechanisms. The first one uses standard back-propagation methods, and the second uses a maximum approximate variational lower bound for training. The model uses a CNN as an encoder to extract a set of feature vectors and use it as input for the attention mechanism. The model uses an LSTM as a decoder to generate caption. LSTM is also one kind of RNN capable of learning long-term dependencies, whereas standard RNN has a short-term memory, which does not work well when a caption contains long texts. Therefore, we might miss essential facts for screenshots. LSTM is a good choice in that situation. We get the final predicted captions based on the hidden state of the LSTM, context vector, and the words that we get from the encoding phase.</p> <p> </p> Fig 2: Example a Mobile GUI Screenshot and corresponding metadata"}, {"location": "Clarity/#mobile-graphical-user-interfaces", "title": "Mobile Graphical User Interfaces", "text": "<p>GUIs of Android apps are composed of both GUI-components and GUI-containers, which together form a GUI-hierarchy. GUI-components represent atomic widgets that carry with them a pre-defined base functionality (e.g. Button, Spinner, NumberPicker). GUI-containers organize GUI-components into logical groups, and define certain spatial or stylistic properties of the contained GUI-components (e.g., grouping together TextViews on a panel with a certain color background).  A hierarchal nesting of GUI-components and GUI-containers forms a rooted tree structure, where a single GUI-container serves as the root of the tree. In Android, this GUI-structure is stipulated in static code by xml files in the resource directory of an Android app (e.g. the /res/layout directory). This same GUI-structure is represented dynamically at runtime by metadata defined by an xml structure which can be read from an Android device using Android's uiautomator framework. Fig. 2 illustrates the runtime xml representation of a Ringtone app. The three highlighted xml snippets represent the corresponding GUI-metdata for the Menu button, as well as the \"New'' card, and the \"Abstract'' card, which allow users to browse various categories of ringtones. In many ways, the GUI-metadata described above constitutes another representation for software GUIs, one that is lexical in nature. Furthermore, it is possible that the information encoded into GUI-metadata is complimentary to what is encoded by pixel-based screenshots. However, it is clear that not all attributes included in GUI-metadata would be useful as a rich representation (i.e., many attribute fields are often just \"false''). Thus, to perform a thorough investigation of the latent potential for different GUI representations to encode functional properties, we explore whether the seq2seq model can effectively translate between differing collections of GUI-metadata attributes and NL descriptions.</p>"}, {"location": "Clarity/#the-clarity-dataset", "title": "The CLARITY Dataset", "text": "Fig 3: Example of Collected Captions from the Clarity dataset"}, {"location": "Clarity/#collecting-the-clarity-dataset", "title": "Collecting the Clarity Dataset", "text": "<p>To collect the Clarity dataset, we utilized the ReDraw dataset of screens and GUI-component information with additional filtering techniques applied to remove landscape screens, blank screens, and screens with semi-transparent overlays. This process resulted in a candidate set of 17,203 screens suitable for the labeling process.</p> <p>Once we derived a suitable set of screens, we needed to manually label these screens with functional captions. After an initial pilot study to confirm the validity of our labeling process we performed a full scale data collection study using Amazon's Mechanical Turk Crowd-worker platform to collect over 10k screens with functional descriptions.</p> <p>Caption Granularity: Intuitively, GUIs encode functional information at multiple levels of granularity. For example, if you were to ask a user or developer what the high-level purpose of a given screen is, they might say, \"This screen allows users to browse clothing categories'', as shown in Fig. 3. These types of descriptions constitute the \u201chigh-level\u201d functionality of a given screen. However, a single screen rarely implements only one functionality, and there may be multiple functional properties that enable the screen's high-level functional purpose. User descriptions of these types of functional properties are typically centered around the interactive components of a screen, since these represent the instances of actions (e.g., users \u201cdoing something\u201d) that are easily attributed to implemented functions. For example, in the screen in Fig. 3, underlying functions include viewing favorites, accessing a shopping cart, or selecting an item from a list. These types of \"low-level'' screen properties centered around GUI-components describe key constituent functionality. Hence, in order to capture a holistic functional view of each screen, we tasked participants with labeling each screen with one \u201chigh-level\u201d functional caption, and up to four \"low-level'' functional captions. Fig. 3 shows these two categories using actual captions collected as part of the Clarity dataset. </p> <p>Mechanical Turk Data Collection Study: After the initial pilot study and additional round of screen inspection and filtering, we collected functional image captions at large scale using MTurk. To set up this data-collection process, we configured a Human Intelligence Task (HIT) that provided workers with a set of detailed instructions, displaying a screenshot from our dataset alongside text entry boxes for one high-level functional caption and up to four low-level functional captions. This study was approved by the Institutional Review Board of the authors' affiliated institution.</p> <p> Click here to view an example Clarity hit </p> <p>MTurk provides various levels of configuration for a given HIT to help target certain populations of users for specific tasks based on demographic information and their qualifications as a crowd worker (e.g., their number of successfully completed HITs). Additionally, you can set the price paid to each worker for satisfactorily completing a given HIT. When a worker completes a HIT, it must be \"accepted'' by the requester before the participant is paid, allowing for control over the quality of submissions from workers. Given that we aimed to collect high-quality functional descriptions of screens in natural English, we targeted MTurk users from primarily English speaking countries that had completed at least 1,000 HITs and had a HIT approval rate of at least 90%. We provided a detailed set of instructions for labeling images with captions that clearly explained the concept of high-level and low-level captions with examples, and provided users with explicit instructions as well as DOs and DONTs for the labeling task. The full set of instructions can be viewed by clicking the button above.  With regard to caption quality, we specifically had three major requirements: (i) that the caption describes the perceived functionality of a screen and not simply its appearance, (ii) that spatial references are given for low-level captions (e.g., \u201cthe button in the top-left corner of the screen\u201d), and (iii) that captions be written in complete English sentences with reasonably proper grammar. We opted to require screen location information for low-level components to investigate whether such spatial relationships could be learned from visual GUI-related data.</p> <p>We published batches of HIT tasks by sampling unique screens from our set of candidate screens, ensuring that no user was assigned the same screen twice. The quality of work from crowd-sourced tasks is not always optimal, so as captions were submitted, they needed to be vetted for quality. To perform quality vetting, we implemented a Java app that displayed screenshots with their corresponding crowdsourced captions, as well as options for rejecting/accepting these captions. Using this tool, The captions for each screen were examined by at least one author for the three quality attributes mentioned above. If an author was unsure about whether a screen met these quality attributes, it was reviewed by at least one other author to reach a consensus. In summary, we collected a total of 45,998 captions (across granularity levels) for 10,204 screens, and paid out over $2,400 to workers.</p>"}, {"location": "Clarity/#clarity-dataset-analysis", "title": "Clarity Dataset Analysis", "text": "<p>The Clarity dataset provides a rich source of data for exploring the relationship between GUI-based and lexical software data. However, it is important to investigate the semantic makeup of the collected captions in order to better understand: (i) the latent topics they capture as well as (ii) their naturalness and, hence, predictability. This type of investigation should lead to insights regarding the ability of such NL descriptions to be appropriately modeled and thus predicted to facilitate program comprehension. In this section we carry out an empirical analysis of this phenomena guided by the following two Research Questions (RQs):</p> <p>RQ1: What are the descriptive latent topics captured within the high- and low-level captions in the Clarity dataset?</p> <p>RQ2: How natural (i.e., predictable) are the high- and low-level captions in the Clarity dataset?</p>"}, {"location": "Clarity/#results-of-clarity-dataset-topic-modeling-rq1", "title": "Results of Clarity Dataset Topic Modeling (RQ1)", "text": "<p> Table 6: LDA Topics Learned Over High Level Captions Assigned Label Top Seven Words Sign Up Page applic screen display sign page use show App Terms screen user provid app inform allow term Color Options screen show app option color book differ Video Game Home screen page show app video game home Login or Create Account user screen allow account log creat app Enter Email and Password user screen enter password email allow address Choose App Language user screen allow choos access app languag Select Image from List user screen allow select view list imag Map Search by Location screen locat search map user show find Facebook Login Screen page screen app show login websit facebook <p></p> <p> Table 7: LDA Topics learned over Low-Level Captions Assigned Label Top Seven Words app logo screen app text center bottom indic logo page button page button top center bottom side left game start user click button game bottom view start select date avail date select one option theme present wallpaper options set screen show option wallpap ad chang top left back button button user screen left top corner back camera button video imag photo pictur screen bottom camera popup cancel button button popup cancel bottom screen right box name field first second row third last name question privacy policy banner titl just term blue banner privaci polici <p></p> <p> Click here to view all topic modeling results</p> <p>We present selected results of some of the most representative topics in Tables 6 &amp; 7, complete with descriptive labels that we provide for readability, whereas all results can be downloaded by clicking the button above. These topics help to provide a descriptive illustration of some of the latent patterns that exist in both the high and low level Clarity captions. The high-level captions illustrate several screen level topics, including searching on a map and adjusting app settings. The low-level captions conversely capture topics that describe component-level functionality, such as date selectors, camera buttons, and back buttons. These results indicate the existence of logical topics specific to the domain of GUIs in our collected captions.</p>"}, {"location": "Clarity/#results-of-n-gram-language-modeling-on-the-clarity-dataset-rq2", "title": "Results of n-gram Language Modeling on the CLARITY Dataset (RQ2)", "text": "Fig 4: N-gram Cross Entropy for High- and Low-Level Clarity datasets compared with other English and software-related corpora <p>The results of our naturalness analysis are illustrated in Figure 4. This figure shows the average cross entropy of the high- and low- level captions from the Clarity dataset compared to several other corpora as calculated by Rahman. More specifically, the graph depicts the average ten-fold cross entropy for: (i) The Gutenberg corpus containing over 3k English books written by over a hundred different authors, (ii) Java code from over 134 open source projects on GitHub, (iii) Java without Syntax Tokens (i.e., separators, keywords, and operators), and (iv) a Stack Overflow corpus consisting of only the English descriptions from over 200k posts.</p> <p>As described earlier, the lower the cross-entropy is for a particular dataset, the more natural it is. That is, the corpora that exhibit lower cross entropy tend to exhibit stronger latent patterns that can be effectively modeled and predicted. As we see from Fig. 4, the Clarity high and low level captions are more natural than every dataset excluding raw Java code. This signals that our collected captions exhibit strong semantic patterns that can be effectively modeled for prediction. Additionally, we observe that the cross-entropy for the high and low-level captions are surprisingly similar. Intuitively, one might expect that the low-level Clarity captions would exhibit more prevalent patterns due to the repetitive use cases of certain GUI-components such as menu buttons. However, we see only minor differences between the two corpora. This indicates the proclivity of both datasets to exhibit patterns that can be appropriately modeled.</p>"}, {"location": "Clarity/#empirical-study", "title": "Empirical Study", "text": "<p>We aim to learn whether state-of-the-art DL models can appropriately predict a natural language description of functionality given both pixel-based and metadata-based GUI information. Thus, we perform a comprehensive empirical evaluation with two main goals: (i) intrinsically evaluate the predictive power of the models according to well accepted machine translation effectiveness metrics, and (ii) extrinsically evaluate the models by examining and rating the quality of the predicated functional NL descriptions. The quality focus of this evaluation is our studied models' ability to effectively predict accurate, concise, and complete functional descriptions. To aid in achieving the goals of our study, we define the following RQs:</p> <p>RQ3: How accurate are our studied model\u2019s predicted natural language descriptions?</p> <p>RQ4: How effective, complete, and understandable are our model\u2019s predicted NL descriptions from the viewpoint of evaluators?</p>"}, {"location": "Clarity/#image-captioning-model-configurations", "title": "Image Captioning Model Configurations", "text": "<p> Table 1: Model Configurations used in the Quantitative Empirical Study Model Identifier Caption Config. Model Config. im2txt im2txt-h-imgnet High inception v3 trained on imagenet im2txt-l-imgnet Low im2txt-c-imgnet Combined im2txt-h-comp High Inception v3 fine-tuned on Component Dataset im2txt-l-comp Low im2txt-c-comp Combined im2txt-h-fs High Inception v3 fine-tuned on Full Screen Dataset im2txt-l-fs Low im2txt-c-fs Combined NeuralTalk2 ntk2-h-imgnet High VGGNet pre-trained on ImageNet ntk2-l-imgnet Low ntk2-c-imgnet Combined ntk2-h-ft High VGGNet pre-trained on ImageNet with Fine Tuning ntk2-l-ft Low ntk2-c-ft Combined Show, Attend &amp; Tell sat-h High VGGNet pre-trained on ImageNet sat-l Low sat-c Combined <p></p> <p>We train and test the three state-of-the-art image captioning models, im2txt, neuraltalk2, and show, attend and tell (SAT), on the screenshots and captions of the Clarity dataset. We choose to explore these three models due to their different underlying design decisions related to the type of utilized CNNs and RNNs (See background above), as these differences may affect their performance in our domain. These models contain several different configuration options that provide a rich experimental space of their effectiveness. However, given the typical number of parameters that constitute these models, the training time can be quite prohibitive, even on modern hardware. Thus, to control our experimental complexity and investigate a number of model configurations that can be trained in a reasonable amount of time, we fix the values of the hyper-parameters for each model in our experiments. We derived our utilized hyper-parameter values by conducting brief random searches for optimal values of certain parameters, and chose optimal parameters reported in prior work for others. While we fix the hyper-parameters for these models, we instead vary the configurations of our image captioning models at the architecture level. Specifically, we investigate how training the ''encoder'' CNN using different datasets and training procedures affects the efficacy of the model predictions. This type of analysis allows us to more effectively flush out broader patterns related to the benefits and drawbacks of fundamental model design decisions. In the end, we trained more than 15 different configurations of the image captioning models (See Table 1), not including initial parameter searches, over several machine months of computation.</p> <p> Table 2: im2txt Hyperparameters Hyperparameter Value Used Hyperparameter Value Used values_per_input_shard 2300 lstm_dropout_keep_prob 0.7 input_queue_capacity_factor 2 num_examples_per_epoch 58000 num_input_reader_threads 8 optimizer SGD self.vocab_size 12000 initial_learning_rate 2 .num_preprocess_threads 8 learning_rate_decay_factor 0.5 batch_size 64 num_epochs_per_decay 8 initializer_scale 0.08 train_inception_learning_rate 0.005 embedding_size 512 clip_gradients 5 num_lstm_units 512 max_checkpoints_to_keep 5 <p></p> <p>im2txt Model Configurations &amp; Training: For im2txt, we utilized Google's open source implementation of the model in TensorFlow. Given the incredibly large number of parameters that need to be trained for the im2txt model, performing even relatively simple hyperparamter searches proved to be computationally prohibitive for our experiments. Therefore, for this model we utilized the optimal set of parameters reported by Vinyals et.al. on similarly sized datasets. The hyper parameters utilized for our models are given Table 2. The publicly available implementation of Google's im2txt model utilizes the Inception v3 image captioning architecture as its encoder CNN. In past work, the inception model weights were initialized by training on the large-scale image classification dataset imagenet, which contains thousands of categories representing \u201ccommonplace\u201d subject categories (e.g. tree, cat, dog, etc). However, given that we are applying these models to very particular domain (predicting descriptions of software) it is unclear if an Inception v3 model trained on the broader imagenet dataset would capture subtle semantic patterns in the Clarity dataset. Therefore, we explored three different model configurations to explore this phenomena: one with Inception v3 pre-trained on imagenet, and two with Inception v3 fine-tuned on domain specific-datasets. The first domain specific image dataset we utilize is the ReDraw cropped image dataset, which contains over 190k images of native Android GUI-components labeled with their  type (e.g., Button, TextView). The second domain specific image dataset we use consists of the full screenshots from the Clarity dataset, labeled with their Google Play categories. For the ReDraw cropped image dataset, we utilize the train/val/test split provided by the authors, whereas for the full screenshot dataset, we perform an even 80/10/10 split across labeled categories.</p> <p>The Inception v3 models utilizing our domain specific cropped and full image datasets were trained for 1 million iterations, and achieved 94% and 98% top-1 precisions respectively. The im2txt models were trained on the shared Clarity training set for the high, low, and combined caption configurations for 500k iterations, with checkpoints being saved every 2,000 iterations. Given the prohibitive training time, we did not perform joint-fine tuning of the weights of the CNN and RNN for im2txt. Instead, we explore the effects of joint-fine tuning with the NeuralTalk2 models.</p> <p> Table 3: NeuralTalk2 Hyperparameters Hyperparameter Value Used Hyperparameter Value Used rnn_size 512 optim_beta N/A for sgd input_encoding_size 512 optim_epsilon 1.00E-08 batch_size 16 cnn_learning_rate 1.00E-05 grad_clip 0.1 cnn_weight_decay 0 drop_prob_lm 0.7 cnn_optim sgd max_iters 500 000 cnn_optim_alpha 0.8 learning_rate 2 cnn_optim_beta 0.999 learning_rate_decay_start 10 000 cnn_learning_rate 9.99E-01 learning_rate_decay_every 50 000 finetune_cnn_after 0 optim sgd cnn_weight_decay 0 optim_alpha N/A for sgd seed 123 <p></p> <p>NeuralTalk2 Model Configurations &amp; Training: For neuraltalk2, we utilized Karpathy et.al\u2019s implementation written in Torch and lua. We performed a brief randomized hyper-parameter search for this model, given its more efficient training time, using the optimal im2txt parameters as a starting point . The optimal values resulting from this search are provided in Table 3. For its CNN decoder, neuraltalk2 makes use of a VGGNet architecture pre-trained on the ImageNet dataset. Unlike our im2txt configurations, we explore the effect of jointly fine-tuning neuraltalk2's CNN and RNN. Thus, we explore two configurations of neuraltalk2, one that jointly fine tunes the pre-trained VGGNet on the Clarity dataset, and one that does not perform fine-tuning. We followed a training procedure similar to that of our im2txt models, in that we trained our models on the high, low, and combined Clarity caption training data for 500,000 iterations, saving model weight checkpoints every 2,000 iterations.</p> <p> Table 4: SAT Hyperparameters Hyperparameter Value Used Hyperparameter Value Used dim_embedding 512 learning_rate_decay_factor 1.0 dim_decode_layer 1024 num_steps_per_decay 100000 fc_drop_rate 0.5 clip_gradients 5.0 lstm_drop_rate 0.3 momentum 0.0 attention_loss_factor 0.01 decay 9.0 batch_size 17 beta1 0.9 Optimizer Adam beta2 0.999 initial_learning_rate 0.0001 epsilon 1e-6 num_lstm_units 512 <p></p> <p>Show, Attend and Tell Model Configurations &amp; Training: For the SAT model, we adapted the open-source implementation of the model in Tensorflow. The hyperparameters that we used to train our model are shown in the Table 4 above. The implementation used VGG16 as its encoder CNN. We trained the SAT model on the CLARITY dataset for the low, high and combined captions for 500K iterations and kept the checkpoints after every 1K iterations. Note that due to the prohibitive training cost of this model, we did not explore using a fine-tuned VGGNet as we did with neuraltalk2. </p>"}, {"location": "Clarity/#metadata-captioning-model-configurations", "title": "Metadata Captioning Model Configurations", "text": "<p> Table 5: Seq2Seq Model Configurations Model Identifier Caption Config. Model Config. seq2seq seq2seq-h-type High Trained on GUI Component Types seq2seq-l-type Low seq2seq-c-type Combined seq2seq-h-text High Trained on GUI-Component Text seq2seq-l-text Low seq2seq-c-text Combined seq2seq-h-tt High Trained on GUI-Component Type + Text seq2seq-l-tt Low seq2seq-c-tt Combined seq2seq-h-ttl High Trained on GUI-component Type + Text + Location seq2seq-l-ttl Low seq2seq-c-ttl Combined <p></p> <p>     Table 6: Seq2Seq Hyperparameters      General Hyperparameters Value Used Encoder parameters Value Used embedding.dim  128      cell_class   GRUCell  encoder.class  BidirectionalRNNEncoder num_units   128  decoder.class   AttentionDecoder   dropout_input_keep_prob   0.8  optimizer.name   Adam      dropout_output_keep_prob  1  optimizer.learning_rate   0.0001      num_layers    1  source.max_seq_len   50    Decoder Parameters  Value Used  source.reverse   FALSE    cell_class    GRUCell  target.max_seq_len 50    num_units    128  Attention Parameters   Value Used   dropout_input_keep_prob      0.8  num_units   128   dropout_output_keep_prob     1  num_layers    1   Optimizer Parameters  epsilon 0.0000008 <p> </p> <p>To explore the ability to translate between the lexical representations of GUI-metadata and NL functional descriptions, we train and test an encoder-decoder neural language model using Google's seq2seq framework. We chose to utilize the default general-purpose architecture and hyper-parameters for this model, as they have been shown to be effective across a wide-range of machine translation tasks. More specifically, our encoder network consists of a BRNN composed of Gated Recurrent Units (GRUs) and our decoder network consists of an RNN composed of LSTM units. The hyper-parameters used in training this model are given in Table 6.</p> <p>In contrast to the image captioning models, where the input to the encoder is an image, this model takes as input information from GUI metadata and predicts, or \"translates\" the corresponding NL caption. However, as illustrated in the Background, metadata-based representations of GUIs contain several attribute fields, not all of which may be helpful in learning latent semantic patterns. Therefore, to investigate the representative power of different attributes included in Android GUI-metadata, we create four configurations of GUI-metadata consisting of different attribute combinations (Table 5). We chose to utilize these attribute combinations as they represent (i) the attributes that are most likely to have values, and (ii) represent a wide range of information types (e.g., displayed text, component types, and spatial information). Each of these four sets is paired with the high, low, and combined sets of captions (Table 5), and segmented according to training, validation, and test sets consistent across all models. Consistent with the training procedures of the other models, our implementation of the seq2seq model was trained to 500k iterations, with checkpoints being saved every 2k iterations.</p>"}, {"location": "Clarity/#empirical-results", "title": "Empirical Results", "text": ""}, {"location": "Clarity/#results-from-quantitative-model-evaluation-rq3", "title": "Results from Quantitative Model Evaluation (RQ3)", "text": "<p>The two tables below illustrate the BLEU score results for all model combinations trained (in the paper we only show the best performing model for readability\u2019s sake). The best performing model configuration for each type of model are highlighted in blue.</p> <p>         Table 7: BLEU Scores of All Image Captioning Modles on the Clarity Test Set (Rows highlighted in blue are the best performing configuration for each model type) Model Name Caption Configuration Model Configuration Composite BLEU Score BLEU-1 BLEU-2 BLEU-3 BLEU-4 im2txt-c-imgnet Combined Inception v3 Pre-Trained on ImageNet 0.302858     0.520607  0.360747     0.217637     0.112439  im2txt-h-imgnet High-Level 0.123471     0.250284  0124310   0.066160     0.053129    im2txt-l-imgnet Low-Level 0.269517     0.457925  0.315951   0.199971     0.104220     im2txt-c-comp      Combined   Inception v3 Fine Tuned on ReDraw Cropped Images    0.302978    0.516608      0.359123  0.220528   0.115653  im2txt-h-comp      High-Level   0.116127 0.242234     0.116822     0.057997     0.047454   im2txt-l-comp     Low-Level  0.270481 0.456676   0.318999   0.200112   0.106137  im2txt-c-fs    Combined  Inception v3 Fine-Tuned on ReDraw Full Screenshots 0.300721  0.513505   0.359870   0.216181   0.113326 im2txt-h-fs   High-Level   0.123994    0.248933   0.126139   0.067528   0.053376 im2txt-l-fs  Low-Level    0.264500  0.449909     0.312707     0.188958     0.106428     ntk2-c-imgnet    Combined     VGGNet Pre-Trained on ImageNet No Joint Fine Tuning  0.297983      0.524647      0.35956  0.210721     0.097001   ntk2-h-imgnet      High-Level    0.133537    0.273495   0.135113   0.072640   0.052902 ntk2-l-imgnet    Low-Level    0.273620  0.470703     0.325110     0.200512     0.098157 ntk2-c-ft   Combined  VGGNet model pre-trained on ImageNet with Joint Fine Tuning  0.301568    0.520519   0.360115   0.217828   0.107810 ntk2-h-ft      High-Level   0.126834  0.266716     0.125605     0.068949     0.046064    ntk2-l-ft    Low-Level  0.273822  0.474743    0.328862     0.195099   0.096584  sat-c  Combined   VGGNet model pre-trained on ImageNet with Joint Fine Tuning  0.377    0.568    0.420  0.305  0.220    sat-h    High-Level     0.177   0.301  0.183  0.129  0.098  sat-l  Low-Level     0.350    0.525  0.387  0.281  0.207  <p> </p> <p>         Table 8: BLEU Scores for all Metadata Captioning Models on Clarity Test Set (Rows highlighted in blue are the best performing configuration each model type.) Model Name Caption Type Model Configuration Composite BLEU Score BLEU-1 BLEU-2 BLEU-3 BLEU-4 seq2seq-h-type   Combined     Trained on UI Component Types  0.169    0.389    0.147    0.060    0.080 seq2seq-l-type     High-Level     0.174  0.407  0.140  0.085  0.065  seq2seq-c-type     Low-Level  0.181  0.446  0.170  0.079  0.029  seq2seq-h-text   Combined     Trained on UI Component Text      0.132    0.322    0.125    0.058    0.024  seq2seq-l-text      High-Level   0.141    0.401    0.0139   0.084    0.065  seq2seq-c-text    Low-Level   0.156    0.369    0.152    0.075    0.030  seq2seq-h-tt     Combined     Trained on UI Component Type + Text      0.119    0.303    0.108    0.049    0.019  seq2seq-l-tt     High-Level   0.162    0.374    0.134    0.080    0.060  seq2seq-c-tt      Low-Level    0.133    0.323    0.132    0.057    0.020  seq2seq-h-ttl    Combined     Trained on UI Component Type + Text + Location   0.116    0.291    0.113    0.044    0.017  seq2seq-l-ttl      High-Level   0.161    0.384    0.128    0.076    0.057  seq2seq-c-ttl      Low-Level    0.136    0.325    0.136    0.061    0.024  <p> </p>"}, {"location": "Clarity/#predicted-captions-for-best-model-configurations", "title": "Predicted Captions for Best Model Configurations", "text": "<p>Below you can find all of the predicted captions for the im2txt, NueralTalk2, and Seq2Seq Models, as indicated by the results illustrated in Tables 7 &amp; 8. Simply click on the link to each caption type to be brought to an anonymous external page that allows you to view the predicted captions alongside their target images.</p> <ul> <li>Im2txt Results</li> <ul> <li> High Level </li> <li> Low Level </li> <li> Combined </li> </ul> <li>Neuraltalk2 Results</li> <ul> <li> High Level </li> <li> Low Level </li> <li> Combined </li> </ul> <li>Seq2Seq Results</li> <ul> <li> High Level </li> <li> Low Level </li> <li> Combined </li> </ul> <li>Reference (Original) Captions</li> <ul> <li> High Level </li> <li> Low Level </li> </ul> </ul>"}, {"location": "Clarity/#results-from-qualitative-model-evaluation-rq4", "title": "Results from Qualitative Model Evaluation (RQ4)", "text": "<p>To qualitatively evaluate our studied model's generated captions, we performed a large-scale study involving 220 participants recruited from MTurk. We randomly sampled 220 screens from the Clarity test set, predicted high, low, and combined captions for them using the optimal configurations of im2txt, NeuralTalk2, and seq2seq according to the composite BLEU score for each model and caption level combination. We created a HIT wherein each participant viewed 11 screenshots paired with captions. Two of the 11 captions were reference high and low to serve as a control, while the other 9 captions came from the model predictions. Screens and caption pairs were arranged into HITs such that 1) no single HIT had two of the same screenshot, 2) each of the 11 types of captions (2 reference, 9 model) were included only once per HIT. The order of these captions was randomized per HIT to prevent bias in introduced by identical caption ordering between HITs. By this arrangement, each screen-caption pair was evaluated by 11 participants. After viewing these screenshot-caption pairs,  participants were asked to answer several evaluation questions (EQs), denoted in Table 9, which were adapted from prior work that assessed the quality of automatically generated code summaries. Similar to the Clarity dataset collection, each participant's response was thoroughly vetted by at least one author, and were discarded if the free-response answers were found to be incomplete or inadequate. Responses were collected until 220 HITs were completed by unique respondents.</p> <p> Click here to view an example Study hit </p> <p> Table 9: Questions asked to participants in the qualitative user study QID Question Text Response Type EQ1 This description accurately describes the functionality of this screenshot. Likert Scale EQ2 Considering the content of the description, do you think that the description has no/some/a lot of unnecessary information? Three options EQ3 Considering the content of the description, do you think that the description is easy/moderate/difficult to understand? Three options EQ4 What aspects of the description are accurate? Free response EQ5 What aspects of the description are not accurate? Free response EQ6 How could this description be improved? Free response <p></p> <p> </p> Fig. 5: Results of EQ1 from Qualitative Evaluation <p> </p> Fig. 6: Results of EQ2 from Qualitative Study <p> </p> Fig. 7 Results of EQ3 from Qualitative Study <p>The results of EQ1 are summarized in Fig. 5, EQ2 in Fig. 6, and EQ3 in Fig. 7. The responses to EQ4-6 varied by the type of caption, and are provided as a download below. Respondents consistently verified the accuracy of the reference captions, suggesting only minor improvements such as grammar and additional functionality omitted in the original captions (e.g. \"Call the UI element a button, not an icon. Mention that it's in a pop up dialogue\". For captions generated from the deep learning models, im2txt fared the best in terms of accuracy, followed by neuraltalk2 and seq2seq respectively. For im2txt and neuraltalk2, in many cases respondents verified that the caption was accurate (e.g. \"The description accurately describes the screen, it is in fact a terms and conditions screen.\" and suggested minor improvements similarly to the reference captions (e,g, \"It could add specifics about what the settings pertain to (i.e. security)\"). However, with seq2seq, caption accuracy was much lower, and respondents suggested more drastic revisions of captions, often times writing entirely new captions for EQ6 (e.g. \"It should say 'There are two buttons at the top of the screen.  One allows you to sign in.  The other allows you to create a new account'.\"). For all of the deep learning models, accuracy was highest for the combined and low models, which corresponds to the much larger datasets used to train them.</p> <p> Click Here to Download the User Responses to EQ4-EQ6 </p>"}, {"location": "Clarity/#dataset-and-code", "title": "Dataset and Code", "text": "<p> Click Here to Access the Clarity Dataset </p> <p> Click Here to Access Experimental Data for SANER'22 Paper </p>"}, {"location": "android-dev-tutorial/", "title": "Android Development Tutorial", "text": ""}, {"location": "android-dev-tutorial/#tutorial-materials", "title": "Tutorial Materials", "text": ""}, {"location": "android-dev-tutorial/#tutorial-video", "title": "Tutorial Video", "text": ""}, {"location": "android-dev-tutorial/#tutorial-slides", "title": "Tutorial Slides", "text": "<p>Click on the button below to download the tutorial slides.</p> Click Here to Download Tutorial Slides"}, {"location": "android-dev-tutorial/#getting-started", "title": "Getting Started", "text": "<p>Please follow these instructions before the tutorial begins to install Android Studio (the integrated development environment, or IDE, for building Android Apps) and configure an Android emulator (a virtual device that will allow you to run and test your prototype application).</p>"}, {"location": "android-dev-tutorial/#installing-android-studio", "title": "Installing Android Studio", "text": "<p>Please visit https://developer.android.com/studio in order to download the version of Android studio for the operating system that you use. All major operating systems, including macOS, Windows, and most flavors of Linux are supported.</p> <p>Follow the instructions for your specific operating system to finish the installation. You can find a detailed guide for specific operating systems here.</p>"}, {"location": "android-dev-tutorial/#configuring-your-android-emulator", "title": "Configuring Your Android Emulator", "text": "<p>Once you have installed Android studio, either navigate to the launch screen view or the code editor view in order to launch the AVD Manager. See the screenshots below to see the steps involved in doing this.</p> <p> 1. First click on the \"Configure\" button at the bottom of the splash screen.</p> <p>2. Click on the \"AVD Manager\" option to launch the configurator for the virtual Android Devices. </p> <p> 3. Click on the \"+ Create Virtual Device\" button to launch the configurator wizard.</p> <p> 4. The first screen of the configurator wizard allows you to pick the hardware settings of your virtual Android device. This includes attributes such as the device screen size and pixel density. For the purposes of this tutorial, we will be using the Nexus 5 emulator, which you can select by scrolling to and clicking on the hardware configuration as shown in the above screenshot. After you have selected the Nexus 5 profile, you can click the \"Next\" button.</p> <p> 5. Next we need to configure the software that our Android emulator will run on. The wizard allows you to select from many different Android versions, some of which contain proprietary Google Play APIs. For the purposes of this tutorial, we will be using Android 7.0 (also called Nougat) as it has good compatibility across a range of Android devices. To use this version, you first click on the Download button, to download the software image. After the download is complete, you can then select the software version in the configuration wizard and click the \"Next\" button.</p> <p> 6. The Final screen of the configuration wizard provides some more advanced options and provides general information about the device you configured. For the purposes of this tutorial, we do not need to worry about advanced options, so you can simply click on the \"Finish\" button.</p> <p> 7. Now that your Android emulator has been configured, you can launch the emulator by clicking on the green \"play button as indicated above. This should launch your emulator and allow you to test applications! </p>"}, {"location": "android-dev-tutorial/#getting-help", "title": "Getting Help", "text": "<p>If you have trouble or need help, please reach out to Dr. Moran on the <code>android-tutorial</code> channel in the REU Slack, or the NRT Slack Channel. He will be happy to assist you!</p>"}, {"location": "android-dev-tutorial/#tutorial-developing-a-todo-list-app", "title": "Tutorial: Developing a ToDo List App", "text": "<p>Today, you will be developing a To-Do List application for Android using the Java programming language. While, as discussed in the lecture, Android does support Kotlin, most of you are likely more familiar with Java and it is still supported :-) I have already coded this project and placed it on GitHub, which you can access via the button below. However, you should use this only if you get stuck, otherwise, follow the tutorial below, as it will teach you about the purpose of the code.</p> Click Here to Access the GitHub project <p>Note</p> <p>This tutorial is adapted and updated from Aldo Ziflaj's awesome tutorial, which you can find here (but the code and structure is a bit outdated).</p>"}, {"location": "android-dev-tutorial/#step-1-create-a-new-android-studio-project", "title": "Step 1: Create a New Android Studio Project", "text": "<p>Open the AndroidStudio launch screen and select the \"Create New Project\" button.</p> <p></p> <p>Next select the \"Empty Activity\" and then click \"Next\".</p> <p></p> <p>On the next screen, you can configure the general project attributes. First, choose a suitable name for your TodoList project. You can use any name you like. Next, provide a package name. If you are not sure what to use, you can use <code>edu.gmu.&lt;project-name&gt;</code>. Next, select the location where you want the code to be stored on your computer hard-disk. After that, make sure that the selected Language is <code>Java</code> as we will be using Java for this tutorial. Next, set the minimum SDK to <code>API 24: Android 7.0 (Nougat)</code>. Make sure the \"Use legacy android.support libraries is unchecked. Finally, click the \"Finish\" button to create your project!</p> <p></p> <p>Next, let's make sure your app can run! Once Android Studio finishes automatically setting up the new project, you can click on the green arrow in the menu bar to run your app. This should launch your configured emulator and launch the app!</p> <p></p>"}, {"location": "android-dev-tutorial/#step-2-setting-up-the-ui", "title": "Step 2: Setting up the UI", "text": "<p>In the <code>MainActivity.java</code> class, you should see something like the code below:</p> <pre><code>public class MainActivity extends AppCompatActivity {\n\n    @Override\n    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n    }\n}\n</code></pre> <p>On line 11, you set the view of this activity to <code>R.layout.activity_main</code>, which points to a file called <code>activity_main.xml</code> in the <code>/res/layout</code> directory of the project. To view the layout, select the <code>activity_main.xml</code> file from under the <code>res/layout</code> folder and double click it. This will bring out the layout inspector. However, we will be using the xml code to build this UI. To view the xml code, click on the \"Code\" button in the top left of the LayoutInspector. You will then see a A vie, that controls layout of the Android interface, that looks like this:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:paddingBottom=\"@dimen/activity_vertical_margin\"\n    android:paddingLeft=\"@dimen/activity_horizontal_margin\"\n    android:paddingRight=\"@dimen/activity_horizontal_margin\"\n    android:paddingTop=\"@dimen/activity_vertical_margin\"\n    tools:context=\"com.aziflaj.todolist.MainActivity\"&gt;\n\n    &lt;TextView\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        android:text=\"Hello World!\" /&gt;\n&lt;/RelativeLayout&gt;\n</code></pre> <p>In the <code>main</code> view, you will add a <code>ListView</code>, which will contain a ToDo item in each row. To do this, replace the <code>TextView</code> element with the code below:</p> <pre><code>&lt;ListView\n        android:id=\"@+id/list_todo\"\n        android:layout_width=\"wrap_content\"\n        android:layout_height=\"wrap_content\"\n        app:layout_constraintBottom_toBottomOf=\"parent\"\n        app:layout_constraintLeft_toLeftOf=\"parent\"\n        app:layout_constraintRight_toRightOf=\"parent\"\n        app:layout_constraintTop_toTopOf=\"parent\" /&gt;\n</code></pre> <p>Now you will define a list item, which will represent a task in the interface.</p> <p>Create a new layout file in the <code>/res/layout</code> folder called <code>item_todo.xml</code>. You can do this by right clicking on the <code>layout</code> directory under the <code>res</code> directory and selecting the \"New Layout Resource File\" option.</p> <p>You will add two elements to this file, a <code>TextView</code> to show the task, and a \u201cDone\u201d <code>Button</code> to delete the task. Add this code to <code>item_todo.xml</code>, replacing anything that is already there.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;androidx.constraintlayout.widget.ConstraintLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"&gt;\n\n    &lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n        android:layout_width=\"match_parent\"\n        android:layout_height=\"match_parent\"\n        android:layout_gravity=\"center_vertical\"&gt;\n\n        &lt;TextView\n            android:id=\"@+id/task_title\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:layout_alignParentLeft=\"true\"\n            android:layout_alignParentStart=\"true\"\n            android:text=\"Hello\"\n            android:textSize=\"20sp\" /&gt;\n\n        &lt;Button\n            android:id=\"@+id/task_delete\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:layout_alignParentEnd=\"true\"\n            android:layout_alignParentRight=\"true\"\n            android:text=\"Done\"/&gt;\n\n    &lt;/RelativeLayout&gt;\n\n&lt;/androidx.constraintlayout.widget.ConstraintLayout&gt;\n</code></pre> <p>The app needs a menu item to allow user to add more tasks. Add a <code>main_menu.xml</code> file in the <code>/res/menu</code> directory, following the same procedure as above. The only difference is you will first create a <code>menu</code> directory under the <code>res</code> directory and right click the <code>res/menu</code> directory to add the Layout resource. Once you have created the <code>main_menu.xml</code> layout, double click it to open it, switch to the Code view by clicking the button in the top right, and replace your code with the following:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n\n    &lt;menu xmlns:android=\"http://schemas.android.com/apk/res/android\"\n        xmlns:app=\"http://schemas.android.com/apk/res-auto\"&gt;\n        &lt;item\n            android:id=\"@+id/action_add_task\"\n            android:icon=\"@android:drawable/ic_menu_add\"\n            android:title=\"Add Task\"\n            app:showAsAction=\"always\" /&gt;\n    &lt;/menu&gt;\n</code></pre> <p>Next, we need to connect this static UI code to the main cod logic of the app. To do this, you should add the following information to your <code>MainActivity.java</code> file, after the <code>onCreate</code> method.</p> <pre><code>@Override\npublic boolean onCreateOptionsMenu(Menu menu) {\n    getMenuInflater().inflate(R.menu.main_menu, menu);\n    return super.onCreateOptionsMenu(menu);\n}\n\n@Override\npublic boolean onOptionsItemSelected(MenuItem item) {\n    switch (item.getItemId()) {\n        case R.id.action_add_task:\n            Log.d(TAG, \"Add a new task\");\n            return true;\n\n        default:\n            return super.onOptionsItemSelected(item);\n    }\n}\n</code></pre> <p>Note</p> <p>You might have some errors related to <code>imports</code> when you copy and paste code throughout this tutorial. To resolve these, simply hover your mouse over the red-underlined text, and then click on <code>import class</code>. This will add an <code>import</code> statement to the top of the class so that the code can properly utilize the referenced library class.</p> <p>You will likely have an error after this relating to a <code>TAG</code> constant. To resolve this error, add the following line to the beginning of the <code>MainActivity</code> class: </p> <pre><code>@Override\npublic boolean onCreateOptionsMenu(Menu menu) {\n    getMenuInflater().inflate(R.menu.main_menu, menu);\n    return super.onCreateOptionsMenu(menu);\n}\n\n@Override\npublic class MainActivity extends AppCompatActivity {\n    private static final String TAG = \"MainActivity\";\n...\n</code></pre> <p>The <code>onCreateOptionsMenu()</code> method inflates (renders) the menu in the main activity, and uses the <code>onOptionsItemSelected()</code> method to react to different user interactions with the menu item(s). If you run the application, it should look something like this:</p> <p></p> <p>If you click the add button, you will see something like this in the Android Studio log:</p> <p><code>03-26 22:12:50.327 2549-2549/? D/MainActivity: Add a new task</code></p> <p>Next, you will add an AlertDialog to get the task from the user when the add item button is clicked. You already know where to add the code to react to the user, so replace the logging statement in the <code>MainActivity</code> (depicted below)</p> <pre><code>Log.d(TAG, \"Add a new task\");\n</code></pre> <p>with the following code snippet:</p> <pre><code>final EditText taskEditText = new EditText(this);\nAlertDialog dialog = new AlertDialog.Builder(this)\n        .setTitle(\"Add a new task\")\n        .setMessage(\"What do you want to do next?\")\n        .setView(taskEditText)\n        .setPositiveButton(\"Add\", new DialogInterface.OnClickListener() {\n            @Override\n            public void onClick(DialogInterface dialog, int which) {\n                String task = String.valueOf(taskEditText.getText());\n                Log.d(TAG, \"Task to add: \" + task);\n            }\n        })\n        .setNegativeButton(\"Cancel\", null)\n        .create();\ndialog.show();\n</code></pre> <p>Now if you launch your app again, clicking the plus button should give you this:</p> <p></p> <p>Enter some text and when you click the add button, the Android Studio log (\u201clogcat\u201d) will show something like this:</p> <p><code>03-26 23:32:18.294 12549-12549/? D/MainActivity: Task to add: I want to learn Android Development</code></p>"}, {"location": "android-dev-tutorial/#step-3-storing-and-retrieving-data", "title": "Step 3: Storing and Retrieving Data", "text": "<p>Android ships with an embedded <code>SQLite</code> database. The database needs a table before it can store any tasks, which we will call the \u201cTaskTable\u201d. Create a new <code>db</code> package in the same location as MainActivity.java by click on the package path (e.g., <code>edu.gmu.&lt;your-app.</code>, select \"New Package\" and then call it <code>db</code>. Then create a new class called <code>TaskContract</code> with the file name TaskContract.java by right click on the db folder and selecting \"New Java Class\".</p> <p>Add this code to TaskContract.java, beneath the package declaration.</p> <pre><code>import android.provider.BaseColumns;\n\npublic class TaskContract {\n    public static final String DB_NAME = \"com.aziflaj.todolist.db\";\n    public static final int DB_VERSION = 1;\n\n    public class TaskEntry implements BaseColumns {\n        public static final String TABLE = \"tasks\";\n\n        public static final String COL_TASK_TITLE = \"title\";\n    }\n}\n</code></pre> <p>The <code>TaskContract</code> class defines constants which used to access the data in the database. You also need a helper class called <code>TaskDbHelper</code> to open the database. Create this class in the <code>db</code> package (following the same method as above) and add the following code:</p> <pre><code>   public class TaskDbHelper extends SQLiteOpenHelper {\n\n   public TaskDbHelper(Context context) {\n        super(context, TaskContract.DB_NAME, null, TaskContract.DB_VERSION);\n    }\n\n    @Override\n    public void onCreate(SQLiteDatabase db) {\n        String createTable = \"CREATE TABLE \" + TaskContract.TaskEntry.TABLE + \" ( \" +\n                TaskContract.TaskEntry._ID + \" INTEGER PRIMARY KEY AUTOINCREMENT, \" +\n                TaskContract.TaskEntry.COL_TASK_TITLE + \" TEXT NOT NULL);\";\n\n        db.execSQL(createTable);\n    }\n\n    @Override\n    public void onUpgrade(SQLiteDatabase db, int oldVersion, int newVersion) {\n        db.execSQL(\"DROP TABLE IF EXISTS \" + TaskContract.TaskEntry.TABLE);\n        onCreate(db);\n    }\n}\n</code></pre> <p>Now we need to adapt <code>MainActivity</code> to store data in the database. Add this code where you defined the <code>DialogInterface.OnClickListener()</code> for the AlertDialog\u2018s add button, replacing:</p> <pre><code>String task = String.valueOf(taskEditText.getText());\nLog.d(TAG, \"Task to add: \" + task);\n</code></pre> <p>with: </p> <pre><code>String task = String.valueOf(taskEditText.getText());\n                                SQLiteDatabase db = mHelper.getWritableDatabase();\n                                ContentValues values = new ContentValues();\n                                values.put(TaskContract.TaskEntry.COL_TASK_TITLE, task);\n                                db.insertWithOnConflict(TaskContract.TaskEntry.TABLE,\n                                        null,\n                                        values,\n                                        SQLiteDatabase.CONFLICT_REPLACE);\n                                db.close();\n</code></pre> <p>This makes the whole <code>onOptionsItemSelected()</code> method look like:</p> <pre><code>@Override\npublic boolean onOptionsItemSelected(MenuItem item) {\n    switch (item.getItemId()) {\n        case R.id.action_add_task:\n            final EditText taskEditText = new EditText(this);\n            AlertDialog dialog = new AlertDialog.Builder(this)\n                    .setTitle(\"Add a new task\")\n                    .setMessage(\"What do you want to do next?\")\n                    .setView(taskEditText)\n                    .setPositiveButton(\"Add\", new DialogInterface.OnClickListener() {\n                        @Override\n                        public void onClick(DialogInterface dialog, int which) {\n                            String task = String.valueOf(taskEditText.getText());\n                            SQLiteDatabase db = mHelper.getWritableDatabase();\n                            ContentValues values = new ContentValues();\n                            values.put(TaskContract.TaskEntry.COL_TASK_TITLE, task);\n                            db.insertWithOnConflict(TaskContract.TaskEntry.TABLE,\n                                    null,\n                                    values,\n                                    SQLiteDatabase.CONFLICT_REPLACE);\n                            db.close();\n                        }\n                    })\n                    .setNegativeButton(\"Cancel\", null)\n                    .create();\n            dialog.show();\n            return true;\n\n        default:\n            return super.onOptionsItemSelected(item);\n    }\n}\n</code></pre> <p>Next, add a private instance of <code>TaskDbHelper</code> in the MainActivity class, directly underneath where we had the tag before:</p> <pre><code>public class MainActivity extends AppCompatActivity {\n    private static final String TAG = \"MainActivity\";\n    private TaskDbHelper mHelper;\n</code></pre> <p>And initialize it in the onCreate() method:</p> <pre><code>protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        mHelper = new TaskDbHelper(this);\n\n    }\n</code></pre> <p>Now we need to fetch all the data from the database and show it in the main view. Get a reference to the ListView created in activity_main.xml file by adding an instance of the ListView:</p> <pre><code>public class MainActivity extends AppCompatActivity {\n    private static final String TAG = \"MainActivity\";\n    private TaskDbHelper mHelper;\n</code></pre> <p>And initialize it in the onCreate() method:</p> <pre><code>public class MainActivity extends AppCompatActivity {\n    private static final String TAG = \"MainActivity\";\n    private TaskDbHelper mHelper;\n    private ListView mTaskListView;\n</code></pre> <p>Initialize the reference by adding this line of code to the <code>onCreate()</code> method, right after creating <code>mHelper</code>:</p> <pre><code>    protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        mHelper = new TaskDbHelper(this);\n        mTaskListView = (ListView) findViewById(R.id.list_todo);\n</code></pre> <p>Next we are going to add a method that handles updating the UI. To do this, add the following method in your <code>MainActivity</code> class:</p> <pre><code>    private void updateUI() {\n        ArrayList&lt;String&gt; taskList = new ArrayList&lt;&gt;();\n        SQLiteDatabase db = mHelper.getReadableDatabase();\n        Cursor cursor = db.query(TaskContract.TaskEntry.TABLE,\n                new String[]{TaskContract.TaskEntry._ID, TaskContract.TaskEntry.COL_TASK_TITLE},\n                null, null, null, null, null);\n        while (cursor.moveToNext()) {\n            int idx = cursor.getColumnIndex(TaskContract.TaskEntry.COL_TASK_TITLE);\n            taskList.add(cursor.getString(idx));\n        }\n\n        if (mAdapter == null) {\n            mAdapter = new ArrayAdapter&lt;&gt;(this,\n                    R.layout.item_todo,\n                    R.id.task_title,\n                    taskList);\n            mTaskListView.setAdapter(mAdapter);\n        } else {\n            mAdapter.clear();\n            mAdapter.addAll(taskList);\n            mAdapter.notifyDataSetChanged();\n        }\n\n        cursor.close();\n        db.close();\n    }\n</code></pre> <p>Add a private <code>ArrayAdatper</code> field to the <code>MainActivity</code> class:</p> <pre><code>public class MainActivity extends AppCompatActivity {\n    private static final String TAG = \"MainActivity\";\n    private TaskDbHelper mHelper;\n    private ListView mTaskListView;\n    private ArrayAdapter&lt;String&gt; mAdapter;\n</code></pre> <p>This <code>ArrayAdapter</code> will help populate the ListView with the data.</p> <p>If you don\u2019t understand the <code>updateUI()</code> method, that\u2019s OK. Instead of logging the tasks, add them into an <code>ArrayList</code> of Strings. It then checks if <code>mAdapter</code> is created or not. If it isn\u2019t, and <code>mAdapter</code> is null, create and set it as the adapter of the <code>ListView</code> (see below code you already have):</p> <pre><code>mAdapter = new ArrayAdapter&lt;&gt;(this,\n        R.layout.item_todo, // what view to use for the items\n        R.id.task_title, // where to put the String of data\n        taskList); // where to get all the data\n\nmTaskListView.setAdapter(mAdapter); // set it as the adapter of the ListView instance\n</code></pre> <p>If the adapter is already created (which implies that it\u2019s assigned to the <code>ListView</code>), clear it, re-populate it and notify the view that the data has changed. This means that the view will repaint on the screen with the new data.</p> <p>To see the updated data, you need to call the <code>updateUI()</code> method every time the underlying data of the app changes. So, add it in two places:</p> <ul> <li>In the onCreate() method, that initially shows all the data</li> <li>After adding a new task using the AlertDialog</li> </ul> <p>See the two code snippets below for where <code>updateUI()</code> should be added:</p> <pre><code>protected void onCreate(Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n        setContentView(R.layout.activity_main);\n\n        mHelper = new TaskDbHelper(this);\n        mTaskListView = (ListView) findViewById(R.id.list_todo);\n\n        updateUI();\n    }\n</code></pre> <pre><code>public void onClick(DialogInterface dialog, int which) {\n                                String task = String.valueOf(taskEditText.getText());\n                                SQLiteDatabase db = mHelper.getWritableDatabase();\n                                ContentValues values = new ContentValues();\n                                values.put(TaskContract.TaskEntry.COL_TASK_TITLE, task);\n                                db.insertWithOnConflict(TaskContract.TaskEntry.TABLE,\n                                        null,\n                                        values,\n                                        SQLiteDatabase.CONFLICT_REPLACE);\n                                db.close();\n                                updateUI();\n</code></pre>"}, {"location": "android-dev-tutorial/#step-4-deleting-tasks", "title": "Step 4: Deleting Tasks", "text": "<p>After finishing a task, it should be deleted from the list.</p> <p>Open the <code>item_todo.xml</code> layout and add this line to the <code>Button</code> tag:</p> <pre><code>        &lt;Button\n            android:id=\"@+id/task_delete\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:layout_alignParentEnd=\"true\"\n            android:layout_alignParentRight=\"true\"\n            android:text=\"Done\"\n            android:onClick=\"deleteTask\"/&gt;\n</code></pre> <p>When the button is clicked, it calls this method <code>deleteTask()</code> in the MainActivity class, which you should now add:</p> <pre><code>    public void deleteTask(View view) {\n        View parent = (View) view.getParent();\n        TextView taskTextView = (TextView) parent.findViewById(R.id.task_title);\n        String task = String.valueOf(taskTextView.getText());\n        SQLiteDatabase db = mHelper.getWritableDatabase();\n        db.delete(TaskContract.TaskEntry.TABLE,\n                TaskContract.TaskEntry.COL_TASK_TITLE + \" = ?\",\n                new String[]{task});\n        db.close();\n        updateUI();\n    }\n</code></pre> <p>Now, clicking the <code>Done</code> button will delete the task from the list and the SQLite database!</p> <p>Now run your app to test it out! You should see functionality similar to that indicated below:</p> <p></p>"}, {"location": "android-dev-tutorial/#final-step-congratulations", "title": "Final Step: Congratulations!", "text": "<p>Congrats on Building your first Android app. This app demonstrates some important features of Android, such as how to set up a UI View, how to connect different components together, and how to store data from an application.</p> <p>If you made this far and you want to explore more, check out the Bonus Tasks you can try below. These tasks have much less direction, and expect that you will look up some information on your own.</p>"}, {"location": "android-dev-tutorial/#bonus-task-1-change-the-look-and-feel-of-the-ui", "title": "Bonus Task 1: Change the Look and Feel of the UI", "text": "<p>For this bonus task, you should use the <code>LayoutInspector</code> to change the look and feel of the UI. For example, try to set the theme to GMU colors, and explore different fonts or layout configurations for the tasks!</p>"}, {"location": "android-dev-tutorial/#bonus-task-2-add-due-dates", "title": "Bonus Task 2: Add Due Dates", "text": "<p>For this Bonus Task, we are challenging you to add due dates to the ToDo list app. This will require modifying the <code>AlertDialog</code> the <code>database</code> and the <code>item_todo</code> layout resource file. Check out the following resource on how to use the <code>DatePicker</code> component.  </p>"}, {"location": "android-dev-tutorial/#bonus-task-3-allow-tasks-with-images", "title": "Bonus Task 3: Allow Tasks with Images", "text": "<p>For this Bonus Task, you should modify the Tasks so that the user can add an image to them, in case they need to reference it later. For this task, check out this SO post that provides some direction on how to get started with this.</p>"}, {"location": "aurora/", "title": "Aurora", "text": ""}, {"location": "aurora/#online-appendix-for-aurora-navigating-ui-tarpits-via-automated-neural-screen-understanding", "title": "Online Appendix for \"AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding\"", "text": ""}, {"location": "aurora/#authors", "title": "Authors", "text": "<ul> <li>Safwat Ali Khan, George Mason University</li> <li>Wenyu Wang, StellarCyber Inc.</li> <li>Yiran Ren, Dragon Testing Technologies Inc.</li> <li>Bin Zhu, Dragon Testing Technologies Inc.</li> <li>Jiangfan Shi, Dragon Testing Technologies Inc.</li> <li>Alyssa McGowan, Thomas Jefferson High School of Science &amp; Technology</li> <li>Wing Lam, George Mason University</li> <li>Kevin Moran, University of Central Florida</li> </ul>"}, {"location": "aurora/#abstract", "title": "Abstract", "text": "<p>Nearly a decade of research in software engineering has focused on automating mobile app testing to help engineers in overcoming the unique challenges associated with the software platform. Much of this work has come in the form of Automated Input Generation tools (AIG Tools) that dynamically explore app screens. However, such tools have repeatedly been demonstrated to achieve lower-than-expected code coverage -- particularly on sophisticated proprietary apps. Prior work has illustrated that a primary cause of these coverage deficiencies is related to so-called tarpits, or complex screens that are difficult to navigate. </p> <p>In this paper, we take a critical step toward enabling AIG tools to effectively navigate tarpits during app exploration through a new form of automated semantic screen understanding. That is, we introduce AURORA, a technique that learns from the visual and textual patterns that exist in mobile app UIs to automatically detect common screen designs and navigate them accordingly. The key idea of AURORA is that there are a finite number of mobile app screen designs, albeit with subtle variations, such that the general patterns of different categories of UI designs can be learned. As such, AURORA employs a multi-modal, neural screen classifier that is able to recognize the most common types of UI screen designs. After recognizing a given screen, it then applies a set of flexible and generalizable heuristics to properly navigate the screen. We evaluated AURORA both on a set of 12 apps with known tarpits from prior work, and on a new set of five of the most popular apps from the Google Play store.  Our results indicate that AURORA is able to effectively navigate tarpit screens, outperforming prior approaches that avoid tarpits by 19.6\\% in terms of method coverage.  Our analysis of the results finds that the improvements can be attributed to AURORA's UI design classification and heuristic navigation techniques.</p>"}, {"location": "aurora/#dataset-and-code", "title": "Dataset and Code", "text": "<p> Click Here to Access the AURORA GitHub Repo with Code &amp; Data </p> <p> Click Here to Access the Full Paper PDF </p>"}, {"location": "crashscope-tutorial/", "title": "CrashScope Tutorial", "text": ""}, {"location": "crashscope-tutorial/#tutorial-materials", "title": "Tutorial Materials", "text": ""}, {"location": "crashscope-tutorial/#tutorial-video", "title": "Tutorial Video", "text": "<p>Coming Soon!!</p>"}, {"location": "crashscope-tutorial/#tutorial-slides", "title": "Tutorial Slides", "text": "Click Here to Download Tutorial Slides <p>For additional technical information, you can download the CrashScope Tool Demo and Research Papers below:</p> Click Here to Download CrashScope Research Paper <p></p> Click Here to Download CrashScope Tool Demo Paper"}, {"location": "crashscope-tutorial/#getting-started", "title": "Getting Started", "text": "<p>Please follow these instructions before the tutorial begins to install Android Studio (the integrated development environment, or IDE, for building Android Apps) and configure an Android emulator (a virtual device that will allow you to run and test your prototype application).</p>"}, {"location": "crashscope-tutorial/#installing-android-studio", "title": "Installing Android Studio", "text": "<p>Please visit https://developer.android.com/studio in order to download the version of Android studio for the operating system that you use. All major operating systems, including macOS, Windows, and most flavors of Linux are supported.</p> <p>Follow the instructions for your specific operating system to finish the installation. You can find a detailed guide for specific operating systems here.</p>"}, {"location": "crashscope-tutorial/#configuring-your-android-emulator", "title": "Configuring Your Android Emulator", "text": "<p>Once you have installed Android studio, either navigate to the launch screen view or the code editor view in order to launch the AVD Manager. See the screenshots below to see the steps involved in doing this.</p> <p> 1. First click on the \"Configure\" button at the bottom of the splash screen.</p> <p>2. Click on the \"AVD Manager\" option to launch the configurator for the virtual Android Devices. </p> <p> 3. Click on the \"+ Create Virtual Device\" button to launch the configurator wizard.</p> <p> 4. The first screen of the configurator wizard allows you to pick the hardware settings of your virtual Android device. This includes attributes such as the device screen size and pixel density. For the purposes of this tutorial, we will be using the Nexus 5 emulator, which you can select by scrolling to and clicking on the hardware configuration as shown in the above screenshot. After you have selected the Nexus 5 profile, you can click the \"Next\" button.</p> <p> 5. Next we need to configure the software that our Android emulator will run on. The wizard allows you to select from many different Android versions, some of which contain proprietary Google Play APIs. For the purposes of this tutorial, we will be using Android 7.0 (also called Nougat) as it has good compatibility across a range of Android devices. To use this version, you first click on the Download button, to download the software image. After the download is complete, you can then select the software version in the configuration wizard and click the \"Next\" button.</p> <p> 6. The Final screen of the configuration wizard provides some more advanced options and provides general information about the device you configured. For the purposes of this tutorial, we do not need to worry about advanced options, so you can simply click on the \"Finish\" button.</p> <p> 7. Now that your Android emulator has been configured, you can launch the emulator by clicking on the green \"play button as indicated above. This should launch your emulator and allow you to test applications! </p>"}, {"location": "crashscope-tutorial/#downloading-the-android-build-tools", "title": "Downloading the Android Build Tools", "text": "<p>In order to properly run CrashScope, you will need to download a set of the Android build tools that allow for automated extraction of app information from apks.</p> <p> 1. First, while at the splash Screen of Android Studio click on the \"configure\" button.</p> <p> 2. Next Click on the SDK Manager menu option.</p> <p> 3. Next, in the new screen that pops up, click on the SDK Tools Tab and then click on the Show Package Details button in the bottom left.</p> <p> 4. Next, you will need to scroll until you come across the Build Tools version <code>25.0.1</code>. Click the check next to this option and then click the Apply button.</p> <p> 5. Finally, click the OK button on the confirmation popup that appears. You have now installed the build tools required by CrashScope.</p>"}, {"location": "crashscope-tutorial/#tutorial-executing-a-test-application-with-crashscope", "title": "Tutorial: Executing a Test Application with CrashScope", "text": "<p>Today, you will be executing a test application using the CrashScope tool. To do this, you will first need to clone the CrashScope Repo, which you can find by clicking the button below. Once you have cloned the project, you can import it into the Eclipse IDE (see note below). </p> <p>Note</p> <p>This tutorial is centered around using Eclipse, however, if you are experienced in setting up Java projects in other IDEs feel free to use your IDE of choice.</p>"}, {"location": "crashscope-tutorial/#step-1-clone-the-crashscope-project-libraries", "title": "Step 1: Clone the CrashScope Project &amp; Libraries", "text": "<p>First you will need to clone the CrashScope GitHub repo. You can access the repo by using the link below.</p> Click Here to Access the GitHub project <p>Next, you will need to download the libraries required to run CrashScope (offered as a separated download due to licensing constraints). You can download the required libraries below. Simply download this and unpack them at the root of the CrashScope repo. The folder should named <code>lib</code>.</p> Click here to Download the CrashScope Libraries <p>Finally, you will need to download a test application to run with CrashScope. We will be using the Mileage application in this tutorial. You can download the Mileage <code>.apk</code> file by clicking the button below.</p> Click here to Download the Test App <p>Once, you have downloaded the Mileage application, put it in the root of the cloned CrashScope repository.</p>"}, {"location": "crashscope-tutorial/#step-2-importing-crashscope-into-eclipse", "title": "Step 2: Importing CrashScope into Eclipse", "text": "<p>First, if you have not done so already, download Eclipse from the Eclipse Project Page, and follow the instructions for installing the IDE on your respective personal computer.</p> <p>Next, you will need to import the project into Eclipse. To do this, click on the Eclipse's File menu, and then click on the Import Project option.</p> <p> You should then see the screen above. Select the Existing Projects into Workspace option and then click Next. </p> <p> On the next screen you should click the Browse button to locate the folder where the CrashScope repo was cloned. Then once you have selected that folder, be sure the CrashScope-Execution-Engine project is selected, and click on the Finish button. </p> <p>This should import CrashScope into Eclipse where you can start to use/run it. </p>"}, {"location": "crashscope-tutorial/#step-3-configuring-crashscope", "title": "Step 3: Configuring CrashScope", "text": "<p>In the <code>CrashScope.java</code> class (located in the <code>edu.semeru.android.testing</code> package), you should see something like the code below in the <code>main</code> method:</p> <pre><code>        // TO BE CONFIGURED\n        // This should be a path to a text file that contains a list of apk paths to be run. \n        // Each line should consist of a path to a given apk file.\n        String apkFile = \"apps.txt\";\n\n\n        // TO BE CONFIGURED\n        // This path should point to the aapt tool that is included with the Android SDK.\n        // The current version of CrashScope was tested with version 25.0.1\n        String aaptPath = \"AndroidSDK/sdk/build-tools/25.0.1\";\n\n\n        // TO BE CONFIGURED\n        // This path should point to an empty folder where CrashScope will store some\n        // both temporary data generated during execution, and the final output \n        // execution json files.\n        String dataFolder = \"CrashScope-Data/\";\n</code></pre> <p>The first thing you will need to do is replace these paths with the corresponding paths on your own personal computer according to the provided documentation. </p> <p>Note</p> <p>Be sure that you add the full path of the Mileage application to the first line of the <code>apps.txt</code> file, so that CrashScope knows where to find the app.</p> <p>Additionally, in the same class, in the <code>runCrashScopeLocal</code> method you will see the following:</p> <pre><code>        // TO BE CONFIGURED\n        // This is the path the scripts folder in the crashscope execution engine project.\n        // Should be updated to the appropriate path once you clone the project.\n        String scriptsPath = \"crashscope-execution-engine/scripts\";\n\n        // TO BE CONFIGURED\n        // This is the path to the root of your Android SDK folder.\n        // CrashScope uses various tools included with the Android SDK to ineract with devices and emualtors.\n        String androidSDKPath = \"/Applications/AndroidSDK/sdk\";\n\n\n        // These are the default emulator paths for most systems.\n        // These should only be updated if you are using a non-default emulator path.\n        String avdPort = \"5554\";\n        String adbPort = \"5037\";\n</code></pre> <p>You will also need to do is replace these paths with the corresponding paths on your own personal computer according to the provided documentation.</p>"}, {"location": "crashscope-tutorial/#step-4-running-crashscope", "title": "Step 4: Running CrashScope", "text": "<p>Now that you have configured all the relevant paths, it is time to run CrashScope on the Mileage app!</p> <p> Launch your configured emulator by clicking on the green start button in the Android Studio AVD manager. (Refer to the setup instructions for additional details).</p> <p>Next, you will need to run the <code>main</code> method in the <code>CrashScope.java</code> file. To do this, Right click the <code>CrashScope.java</code> file in the Eclipse File explorer, and then select Run as -&gt; Java Application. </p> <p>This will start the Execution of CrashScope on the Mileage application!</p>"}, {"location": "crashscope-tutorial/#final-step-congratulations", "title": "Final Step: Congratulations!", "text": "<p>Congrats on running your first app with CrashScope. You will find the output of the technique in your <code>CrashScope-Data</code> folder, which will include screenshots of each step, as well as a .json file that contains all of the information for a given execution.</p> <p>We hope that future researchers will build upon CrashScope for AR/VR/XR applicaitons by replacing the <code>uiautomator</code> interface, which fetches information regarding native Android app GUIs, with Computer Vision techniques that can reliably detect various interface components for AR/VR/XR applications.</p>"}, {"location": "publications/", "title": "Publications", "text": "<p> Underlined authors are SAGE Lab members.  Click on a paper title to expand the details.</p> <p> = Journal Publication </p> <p> = Conference Publication </p> <p> = Tool Demo Paper </p> <p> = Dataset Paper</p>"}, {"location": "publications/#2023", "title": "2023", "text": "[ICSE'23] - AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces Paper InfoAbstract <ul> <li> SM Hasan Mansur, Sabiha Salma, Damilola Awofisayo, &amp; Kevin Moran</li> <li> Proceedings of The 45th IEEE/ACM International Conference on Software Engineering (ICSE'23), Melbourne, Australia, May 14th-20th, 2023</li> <li> pdf |  data |  github</li> </ul> <p>Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AIDUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed CONTEXTDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AIDUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.</p> [TestEd'23] - ChatGPT and Software Testing Education: Promises &amp; Perils Paper InfoAbstract <ul> <li>  Sajed Jalil, Suzzana Rafi, Thomas D. LaToza, Kevin Moran, &amp; Wing Lam</li> <li> Proceedings of the 2nd Software Testing Education Workshop (TestEd\u201923), co-located with ICST\u201923, Dublin, Ireland, April 16th, 2023</li> <li> pdf |  data |  github</li> </ul> <p>Over the past decade, predictive language modeling for code has proven to be a valuable tool for enabling new forms of automation for developers. More recently, we have seen the advent of general purpose \"large language models\", based on neural transformer architectures, that have been trained on massive datasets of human written text spanning code and natural language. However, despite the demonstrated representational power of such models, interacting with them has historically been constrained to specific task settings, limiting their general applicability. Many of these limitations were recently overcome with the introduction of ChatGPT, a language model created by OpenAI and trained to operate as a conversational agent, enabling it to answer questions and respond to a wide variety of commands from end-users. The introduction of models, such as ChatGPT, has already spurred fervent discussion from educators, ranging from fear that students could use these AI tools to circumvent learning, to excitement about the new types of learning opportunities that they might unlock. However, given the nascent nature of these tools, we currently lack fundamental knowledge related to how well they perform in different educational settings, and the potential promise (or danger) that they might pose to traditional forms of instruction. As such, in this paper, we examine how well ChatGPT performs when tasked with solving common questions in a popular software testing curriculum. Our findings indicate that ChatGPT can provide correct or partially correct answers in 44% of cases, provide correct or partially correct explanations of answers in 57% of cases, and that prompting the tool in a shared question context leads to a marginally higher rate of correct answers. Based on these findings, we discuss the potential promise, and dangers related to the use of ChatGPT by students and instructors.</p> [ICSE'23 Demo] - BURT: A Chatbot for Interactive Bug Reporting Paper InfoAbstractVideo <ul> <li>  Yang Song, Junayed Mahmud, Nadasheen De Silva, Ying Zhou, Oscar Chaparro, Kevin Moran, Andrian Marcus, &amp; Denys Poshyvanyk</li> <li> Proceedings of the 45th IEEE/ACM International Conference on Software Engineering (ICSE'23), Formal Tool Demonstrations Track, Melbourne, Australia, May 14th-20th, 2023</li> <li> pdf |  data |  github</li> </ul> <p>This paper introduces BURT, a web-based chatbot for interactive reporting of Android app bugs. BURT is designed to assist Android app end-users in reporting high-quality defect information using an interactive interface. BURT guides the users in reporting essential bug report elements, i.e., the observed behavior, expected behavior, and the steps to reproduce the bug. It verifies the quality of the text written by the user and provides instant feedback. In addition, BURT provides graphical suggestions that the users can choose as alternatives to textual descriptions. We empirically evaluated BURT, asking end-users to report bugs from six Android apps. The reporters found that BURT's guidance and automated suggestions and clarifications are useful and BURT is easy to use.</p> <p></p> [ICSE'23 Demo] - AVGUST: A Tool for Generating Usage-Based Tests from Videos of App Executions Paper InfoAbstractVideo <ul> <li>  Saghar Talebipour, Hyojae Park,  Kesina Baral, Leon Yee, Safwat Ali Khan, Kevin Moran, Yuriy Brun, Nenad Medvidovic, &amp; Yixue Zhao</li> <li> Proceedings of the 45th IEEE/ACM International Conference on Software Engineering (ICSE'23), Formal Tool Demonstrations Track, Melbourne, Australia, May 14th-20th, 2023</li> <li> pdf |  data |  github</li> </ul> <p>Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks. We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt's guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.</p> <p></p>"}, {"location": "publications/#2022", "title": "2022", "text": "[ESEC/FSE'22] - AVGUST: Automating Usage-based Test Generation from Videos of App Executions Paper InfoAbstractVideo <ul> <li>  Yixue Zhao, Saghar Talebipour, Kesina Baral, Hyojae Park, Leon Yee, Safwat Ali Khan, Yuriy Brun, Nenad Medvidovic, &amp; Kevin Moran</li> <li> Proceedings of the 2022 ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE'22), Singapore, November 14th-18th, 2022 (22% Acceptance Rate)</li> <li> pdf |  data |  github</li> </ul> <p>Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks. We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt's guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.</p> <p></p> [ESEC/FSE'22] - Toward Interactive Bug Reporting for (Android App) End Users Paper InfoAbstractVideo <ul> <li>  Yang Song, Junayed Mahmud, Ying Zhou, Oscar Chaparro, Kevin Moran, Andrian Marcus, and Denys Poshyvanyk</li> <li> Proceedings of the 2022 ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE'22), Singapore, November 14th-18th, 2022 (22% Acceptance Rate)</li> <li> pdf |  data |  github</li> </ul> <p>Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks. We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt's guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.</p> <p></p> [TSE'22] - Enhancing Mobile App Bug Reporting via Real-time Understanding of Reproduction Steps Paper InfoAbstract <ul> <li>  Mattia Fazzini, Kevin Moran, Carlos Bernal C\u00e1rdenas, Tyler Wendland, Alessandro Orso, and Denys Poshyvanyk </li> <li> IEEE Transactions on Software Engineering (TSE)</li> <li> pdf |  data</li> </ul> <p>Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S+, an automated approach for translating video recordings of Android app usages into replayable scenarios. V2S+ is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user gestures captured in a video, and convert these into a replayable test scenario. Given that V2S+ takes a computer vision-based approach, it is applicable to both hybrid and native Android applications. We performed an extensive evaluation of V2S+ involving 243 videos depicting 4,028 GUI-based actions collected from users exercising features and reproducing bugs from a collection of over 90 popular native and hybrid Android apps. Our results illustrate that V2S+ can accurately replay scenarios from screen recordings, and is capable of reproducing \u2248 90.2% of sequential actions recorded in native application scenarios on physical devices, and \u2248 83% of sequential actions recorded in hybrid application scenarios on emulators, both with low overhead. A case study with three industrial partners illustrates the potential usefulness of V2S+ from the viewpoint of developers.</p> [TSE'22] - Translating Video Recordings of Complex Mobile App UI Gestures into Replayable Scenarios for Native &amp; Hybrid Apps Paper InfoAbstract <ul> <li>  Carlos Bernal C\u00e1rdenas, Nathan Cooper, Madeleine Havranek, Kevin Moran, Oscar Chaparro, Denys Poshyvanyk, and Andrian Marcus</li> <li> IEEE Transactions on Software Engineering (TSE)</li> <li> pdf |  data</li> </ul> <p>Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S+, an automated approach for translating video recordings of Android app usages into replayable scenarios. V2S+ is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user gestures captured in a video, and convert these into a replayable test scenario. Given that V2S+ takes a computer vision-based approach, it is applicable to both hybrid and native Android applications. We performed an extensive evaluation of V2S+ involving 243 videos depicting 4,028 GUI-based actions collected from users exercising features and reproducing bugs from a collection of over 90 popular native and hybrid Android apps. Our results illustrate that V2S+ can accurately replay scenarios from screen recordings, and is capable of reproducing \u2248 90.2% of sequential actions recorded in native application scenarios on physical devices, and \u2248 83% of sequential actions recorded in hybrid application scenarios on emulators, both with low overhead. A case study with three industrial partners illustrates the potential usefulness of V2S+ from the viewpoint of developers.</p> [Computational UI@CHI'22] - Learning Patterns from User Interfaces to Support Software Engineering Tasks Paper InfoAbstract <ul> <li>  Kevin Moran</li> <li> Proceedings of CHI 2022 Workshop on Computational Approaches for Understanding, Generating, and Adapting User Interfaces (Computational UI@CHI\u201922)</li> <li> pdf |  data</li> </ul> <p>In the field of software engineering (SE) research, there has long been a focus on automating various development tasks in an attempt to facilitate or augment the abilities of developers. Research aligned with this objective typically aims to learn models from information mined from software repositories and then apply these models to automate a given SE task. The large majority of this work has focused on artifacts consisting of two main modalities of information -- code and natural language. However, one information source which has been comparatively underutilized is the visual modality of software expressed via User Interfaces (UIs). UIs serve as an important medium of interaction between the logic of an application and users, and as such, they encode salient information about underlying program functionality into rich, pixel-based data representations. Given the latent information contained within UIs, and the rapid advancement of Deep Learning (DL) techniques for computer vision and natural language processing in recent years, there is a tremendous opportunity to leverage UI-related software artifacts to offer novel forms of software development automation. This position paper reflects on past work conducted at the intersection of software engineering and automated reasoning of user interfaces, discusses promising future directions, and potential challenges in enabling new forms of UI-centric automation for developers.</p> [SANER'22] - An Empirical Investigation into the Use of Image Captioning for Automated Software Documentation Paper InfoAbstract <ul> <li>  Kevin Moran, Ali Yachnes, George Purnell, Junayed Mahmud, Michele Tufano, Carlos Bernal-C\u00e1rdenas, Denys Poshyvanyk, and Zach H-Doubler</li> <li> Oral Presentation at the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER\u201922), Honolulu, Hawaii, March 15-18, 2022 (24% Acceptance Rate)</li> <li> pdf |  data</li> </ul> <p>Existing automated techniques for software documentation typically attempt to reason between two main sources of information: code and natural language. However, this reason- ing process is often complicated by the lexical gap between more abstract natural language and more structured programming languages. One potential bridge for this gap is the Graphical User Interface (GUI), as GUIs inherently encode salient information about underlying program functionality into rich, pixel-based data representations. This paper offers one of the first comprehensive empirical investigations into the connection between GUIs and functional, natural language descriptions of software. First, we collect, analyze, and open source a large dataset of functional GUI descriptions consisting of 45,998 descriptions for 10,204 screenshots from popular Android applications. The descriptions were obtained from human labelers and underwent several quality control mechanisms. To gain insight into the representational potential of GUIs, we investigate the ability of four Neural Image Captioning models to predict natural language descriptions of varying granularity when provided a screenshot as input. We evaluate these models quantitatively, using common machine translation metrics, and qualitatively through a large- scale user study. Finally, we offer learned lessons and a discussion of the potential shown by multimodal models to enhance future techniques for automated software documentation.</p> [SANER'22] - An Empirical Investigation into the Reproduction of Bug Reports for Android Apps Paper InfoAbstract <ul> <li> Jack Johnson, Junayed Mahmud, Tyler Wendland, Kevin Moran, Julia Rubin and Mattia Fazzini</li> <li> Oral Presentation at the 29th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER\u201922), Honolulu, Hawaii, March 15-18, 2022 (24% Acceptance Rate)</li> <li> pdf |  data</li> </ul> <p>One of the key tasks related to ensuring mobile app quality is the reporting, management, and resolution of bug reports. As such, researchers have committed considerable resources toward automating various tasks of the bug management process for mobile apps, such as reproduction and triaging. However, the success of these automated approaches is largely dictated by the characteristics and properties of the bug reports they operate upon. As such, understanding mobile app bug reports is imperative to drive the continued advancement of report management techniques. While prior studies have examined high-level statistics of large sets of reports, we currently lack an in-depth investigation of how the information typically reported in mobile app issue trackers relates to the specific details generally required to reproduce the underlying bugs. In this paper, we perform an in-depth analysis of 180 reproducible bug reports systematically mined from Android apps on GitHub and investigate how the information contained in the reports relates to the task of reproducing the described bugs. In our analysis, we focus on three pieces of information: the environment needed to reproduce the bug report, the steps to reproduce (S2Rs), and the observed behavior. Focusing on this information, we characterize failure types, identify the modality used to report the information, and characterize the information quality within the reports. We find that bugs are reported in a multi-modal fashion, the environment is not always provided, and S2Rs often contain missing or non-specific information. These findings carry with them important implications on automated bug reproduction techniques as well as automated bug report management approaches more generally.</p> [TOSEM'22] - A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research - TOSEM'22 Paper InfoAbstract <ul> <li> Cody Watson, Nathan Cooper, David N. Palacio, Kevin Moran, and Denys Poshyvanyk</li> <li> ACM Transactions on Software Engineering &amp; Methodology (TOSEM), accepted</li> <li> pdf |  tool</li> </ul> <p>An increasingly popular set of techniques adopted by software engineering (SE) researchers to automate development tasks are those rooted in the concept of Deep Learning (DL). The popularity of such techniques largely stems from their automated feature engineering capabilities, which aid in modeling software artifacts. However, due to the rapid pace at which DL techniques have been adopted, it is difficult to distill the current successes, failures, and opportunities of the current research landscape. In an effort to bring clarity to this cross- cutting area of work, from its modern inception to the present, this paper presents a systematic literature review of research at the intersection of SE &amp; DL. The review canvases work appearing in the most prominent SE and DL conferences and journals and spans 128 papers across 23 unique SE tasks. We center our analysis around the components of learning, a set of principles that govern the application of machine learning techniques (ML) to a given problem domain, discussing several aspects of the surveyed work at a granular level. The end result of our analysis is a research roadmap that both delineates the foundations of DL techniques applied to SE research, and highlights likely areas of fertile exploration for the future.</p> [Oakland'22] - Why Crypto-detectors Fail: A Systematic Evaluation of Cryptographic Misuse Detection Techniques Paper InfoAbstract <ul> <li> Amit Seal Ami, Nathan Cooper, Kaushal Kafle, Kevin Moran, Denys Poshyvanyk, and Adwait Nadkarni</li> <li> Proceedings of the IEEE Symposium on Security and Privacy (Oakland'22), May  22nd-26th, 2022, to appear, 18 pages</li> <li> pdf |  data</li> </ul> <p>The correct use of cryptography is central to ensuring data security in modern software systems. Hence, several academic and commercial static analysis tools have been developed for detecting and mitigating crypto-API misuse. While developers are optimistically adopting these crypto-API misuse detectors (or crypto-detectors) in their software development cycles, this momentum must be accompanied by a rigorous understanding of their effectiveness at finding crypto-API misuse in practice. This paper presents the MASC framework, which enables a systematic and data-driven evaluation of crypto-detectors using mutation testing. We ground MASC in a comprehensive view of the problem space by developing a data-driven taxonomy of existing crypto-API misuse, containing 105 misuse cases organized among nine semantic clusters. We develop 12 generalizable usage-based mutation operators and three mutation scopes that can expressively instantiate thousands of compilable variants of the misuse cases for thoroughly evaluating crypto-detectors. Using MASC, we evaluate nine major crypto-detectors and discover 19 unique, undocumented flaws that severely impact the ability of crypto-detectors to discover misuses in practice. We conclude with a discussion on the diverse perspectives that influence the design of crypto-detectors and future directions towards building security-focused crypto-detectors by design.</p>"}, {"location": "publications/#2021", "title": "2021", "text": "[NLP4Prog'21] - Code to Comment Translation: A Comparative Study on Model Effectiveness &amp; Errors Paper InfoAbstractVideo <ul> <li> Junayed Mahmud, Fahim Faisal, Raihan Islam Arnob, Antonios Anastasopoulos, and Kevin Moran</li> <li> Proceedings of the First Workshop on Natural Language Processing for Programming (NLP4Prog\u201921) Co-Located with ACL-IJCNLP\u201921, Virtual (originally Bangkok, Thailand), August  6th, 2021, to appear, 9 pages</li> <li> pdf |  data</li> </ul> <p>Automated source code summarization is a popular software engineering research topic wherein machine translation models are em- ployed to \u201ctranslate\u201d code snippets into relevant natural language descriptions. Most evaluations of such models are conducted us- ing automatic reference-based metrics. How- ever, given the relatively large semantic gap between programming languages and natural language, we argue that this line of research would benefit from a qualitative investigation into the various error modes of current state- of-the-art models. Therefore, in this work, we perform both a quantitative and qualitative comparison of three recently proposed source code summarization models. In our quantitative evaluation, we compare the models based on the smoothed BLEU-4, METEOR, and ROUGE-L machine translation metrics, and in our qualitative evaluation, we perform a manual open-coding of the most common errors committed by the models when com- pared to ground truth captions. Our investigation reveals new insights into the relationship between metric-based performance and model prediction errors grounded in an empirically derived error taxonomy that can be used to drive future research efforts.</p> <p></p> [MSR Data Showcase'21] - AndroR2: A Dataset of Manually-Reproduced Bug Reports for Android Apps Paper InfoAbstract <ul> <li> Tyler Wendland, Jingyang Sun, Junayed Mahmud, S.M. Hasan Mansur, Steven Huang, Kevin Moran, Julia Rubin, Mattia Fazzini</li> <li> Proceedings of the 2021 International Conference on Mining Software Repositories (MSR\u201921), Data Showcase Track, Virtual (originally Madrid, Spain), May 17th - 19th, 2021, to appear, 5 pages</li> <li> pdf |  data</li> </ul> <p>Software maintenance constitutes a large portion of the software development lifecycle. To carry out maintenance tasks, developers often need to understand and reproduce bug reports. As such, there has been increasing research activity coalescing around the notion of automating various activities related to bug reporting. A sizable portion of this research interest has focused on the domain of mobile apps. However, as research around mobile app bug reporting progresses, there is a clear need for a large, manually vetted, and reproducible set of real-world bug reports that can serve as a benchmark for future work. This paper presents AndroR2: a dataset of 90 manually reproduced bug reports for Android apps listed on Google Play and hosted on GitHub, systematically collected via an in-depth analysis of 459 reports extracted from the GitHub issue tracker. For each reproduced report, AndroR2 includes an apk file for the buggy version of the app, detailed reproduction steps, an executable reproduction script, and annotations on the quality of the reproduction steps associated with the original report. We believe that the AndroR2 dataset can be used to facilitate research in automatically analyzing, understanding, reproducing, localizing, and fixing bugs for mobile applications as well as other software maintenance activities more broadly in the future.</p> [ICSE'21] - It Takes Two to Tango: Combining Visual and Textual Information for Detecting Duplicate Video-Based Bug Reports Paper InfoAbstractVideo <ul> <li> Nathan Cooper, Carlos Bernal-C\u00e1rdenas, Oscar Chaparro, Kevin Moran, and Denys Poshyvanyk</li> <li> Proceedings of the 43rd International Conference on Software Engineering (ICSE\u201921), Virtual (originally Madrid, Spain), May 25th - 28th, 2021, to appear, 13 pages</li> <li> pdf |  data |  tool</li> </ul> <p>When a bug manifests in a user-facing application, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such information is reported en masse, such as during crowd-sourced testing, managing these artifacts can be a time-consuming process. As the reporting of screen-recordings in particular becomes more popular, developers are likely to face challenges related to manually identifying videos that depict duplicate bugs. Due to their graphical nature, screen-recordings present challenges for automated analysis that preclude the use of current duplicate bug report detection techniques. To overcome these challenges and aid developers in this task, this paper presents Tango, a duplicate detection technique that operates purely on video-based bug reports by leveraging both visual and textual information. Tango combines tailored computer vision techniques, optical character recognition, and text retrieval. We evaluated multiple configurations of Tango in a comprehensive empirical evaluation on 4,860 duplicate detection tasks that involved a total of 180 screen-recordings from six Android apps. Additionally, we conducted a user study investigating the effort required for developers to manually detect duplicate video-based bug reports and compared this to the effort required to use Tango. The results reveal that Tango's optimal configuration is highly effective at detecting duplicate video-based bug reports, accurately ranking target duplicate videos in the top-2 returned results in 83% of the tasks. Additionally, our user study shows that, on average, Tango can reduce developer effort by over 60%, illustrating its practicality.</p> <p></p> [ICSE'21 Demo] - V2S: A Tool for Translating Video Recordings of Mobile App Usages into Replayable Scenarios Paper InfoAbstract <ul> <li> Madeleine Havranek, Carlos Bernal-C\u00e1rdenas, Nathan Cooper, Oscar Chaparro, Denys Poshyvanyk, and Kevin Moran</li> <li> Proceedings of the 43rd International Conference on Software Engineering (ICSE\u201921) - Formal Tool Demonstrations Track, Virtual (originally Madrid, Spain), May 25th - 28th, 2021, to appear, 4 pages (37% Acceptance Rate)</li> <li> pdf |  tool</li> </ul> <p>Screen recordings are becoming increasingly important as rich software artifacts that inform mobile application development processes. However, the amount of manual effort required to extract information from these graphical artifacts can hinder resource-constrained mobile developers. This paper presents Video2Scenario (V2S), an automated tool that processes video recordings of Android app usages, utilizes neural object detection and image classification techniques to classify the depicted user actions, and translates these actions into a replayable scenario. We conducted a comprehensive evaluation to demonstrate V2S's ability to reproduce recorded scenarios across a range of devices and a diverse set of usage cases and applications. The results indicate that, based on its performance with 175 videos depicting 3,534 GUI-based actions, V2S is accurate in reproducing \u224889% of actions from collected videos.</p> [ICSE'21 Demo] - Mutation-based Evaluation of Security-focused Static Analysis Tools for Android Paper InfoAbstract <ul> <li> Amit Seal Ami, Kaushal Kafle, Kevin Moran, Adwait Nadkarni, and Denys Poshyvanyk</li> <li> Proceedings of the 43rd International Conference on Software Engineering (ICSE\u201921) - Formal Tool Demonstrations Track, Virtual (originally Madrid, Spain), May 25th - 28th, 2021, to appear, 4 pages (37% Acceptance Rate)</li> <li> pdf |  tool</li> </ul> <p>This demo paper presents the technical details and usage scenarios of \u03bcSE: a mutation-based tool for evaluating security-focused static analysis tools for Android. Mutation testing is generally used by software practitioners to assess the robustness of a given test-suite. However, we leverage this technique to systematically evaluate static analysis tools and uncover and document soundness issues. \u03bcSE's analysis has found 25 previously undocumented flaws in static data leak detection tools for Android. \u03bcSE offers four mutation schemes, namely Reachability, Complex-reachability, TaintSink, and ScopeSink, which determine the locations of seeded mutants. Furthermore, the user can extend \u03bcSE by customizing the API calls targeted by the mutation analysis. \u03bcSE is also practical, as it makes use of filtering techniques based on compilation and execution criteria that reduces the number of ineffective mutations.</p> [TOPS'21] - Systematic Mutation-based Evaluation of the Soundness of Security-focused Android Static Analysis Techniques Paper InfoAbstract <ul> <li> Amit Seal Ami, Kaushal Kafle, Kevin Moran, Adwait Nadkarni, and Denys Poshyvanyk</li> <li> ACM Transactions on Security &amp; Privacy (TOPS), accepted</li> <li> pdf |  tool</li> </ul> <p>Mobile application security has been a major area of focus for security research over the course of the last decade. Numerous application analysis tools have been proposed in response to malicious, curious, or vulnerable apps. However, existing tools, and specifically, static analysis tools, trade soundness of the analysis for precision and performance and are hence soundy. Unfortunately, the specific unsound choices or flaws in the design of these tools is often not known or well-documented, leading to misplaced confidence among researchers, developers, and users. This paper describes the Mutation-based Soundness Evaluation (\u03bcSE) framework, which systematically evaluates Android static analysis tools to discover, document, and fix flaws, by leveraging the well-founded practice of mutation analysis. We implemented \u03bcSE and applied it to a set of prominent Android static analysis tools that detect private data leaks in apps. In a study conducted previously, we used \u03bcSE to discover 13 previously undocumented flaws in FlowDroid, one of the most prominent data leak detectors for Android apps. Moreover, we discovered that flaws also propagated to other tools that build upon the design or implementation of FlowDroid or its components. This paper substantially extends our \u03bcSE framework and offers an new in-depth analysis of two more major tools in our 2020 study, we find 12 new, undocumented flaws and demonstrate that all 25 flaws are found in more than one tool, regardless of any inheritance-relation among the tools. Our results motivate the need for systematic discovery and documentation of unsound choices in soundy tools and demonstrate the opportunities in leveraging mutation testing in achieving this goal.</p> [TCPS'21] - Security in Centralized Data Store-based Home Automation Platforms: A Systematic Analysis of Nest and Hue Paper InfoAbstract <ul> <li> Kaushal Kafle, Kevin Moran, Sunil Manandhar, Adwait Nadkarni, and Denys Poshyvanyk</li> <li> ACM Transactions on Cyber Physical Systems (TCPS), accepted</li> <li> pdf</li> </ul> <p>Home automation platforms enable consumers to conveniently automate various physical aspects of their homes. However, the security flaws in the platforms or integrated third-party products can have serious security and safety implications for the user\u2019s physical environment. This article describes our systematic security evaluation of two popular smart home platforms, Google\u2019s Nest platform and Philips Hue, which implement home automation \u201croutines\u201d (i.e., trigger-action programs involving apps and devices) via manipulation of state variables in a centralized data store. Our semi-automated analysis examines, among other things, platform access control enforcement, the rigor of non-system enforcement procedures, and the potential for misuse of routines, and it leads to 11 key findings with serious security implications. We combine several of the vulnerabilities we find to demonstrate the first end-to-end instance of lateral privilege escalation in the smart home, wherein we remotely disable the Nest Security Camera via a compromised light switch app. Finally, we discuss potential defenses, and the impact of the continuous evolution of smart home platforms on the practicality of security analysis. Our findings draw attention to the unique security challenges of smart home platforms and highlight the importance of enforcing security by design.</p>"}, {"location": "research/", "title": "Research", "text": ""}, {"location": "research/#research-mission", "title": "Research Mission", "text": "<p>Our research mission is to pursue a more complete understanding of how engineers develop software, and to build the next generation of intelligent developer tools to help facilitate the software engineering process. To accomplish this, we study the methods and techniques by which developers design, create, test, and manage software. In particular, we examine developer needs related to various tasks in the software development lifecycle and design tailored automated approaches for those needs with the intention of facilitating software development and maintenance tasks. Given the developer-focused nature of our research, we aim to straddle the line between scientific discovery and industrial applicability. In other words, we work to identify and understand meaningful problems faced by real developers in the constantly shifting modern landscape of software engineering. We then formulate projects that work towards solving these problems by designing state-of-the-art, automated approaches applying novel adaptations of machine learning (ML), Deep Learning (DL), program analysis, computer vision, and natural language processing (NLP) techniques.</p> <p>In essence, we aim to make it as easy as possible for engineers to go from ideas to working software.</p>"}, {"location": "research/#research-focus-areas", "title": "Research Focus Areas", "text": "Mobile Applications <p>Given their ubiquity in today's society, creating effective developer tools and software engineering practices for mobile applications is essential. Our research looks to build forward-thinking programming tools for mobile apps.</p> AI and Software Engineering <p>Our lab conducts research that aims to leverage new advances in artificial intelligence (with a focus on Deep Learning) to help build automated developer tools. Additionally, we examine how we best design tools and practices to engineer reliable systems that make use of artificial intelligence.</p> Bug Reporting <p>Given the complexity of modern applications, bugs often contnue to persist after release, and users need effective mechanisms to report them. Our research has pioneered interactive bug reporting systems, and we continue to advance error reporting methods for software.</p> Software Security <p>Given that software is interwoven into user's personal lives, the stakes of software security and privacy have never been higher. Our research examines how we can apply software engineering principles and practices to help ensure the security of complex software systems.</p> Automated UI Analysis <p>The Graphical User Interface represents a reich source of information that describes software in a visual manner. They allow users to intuitively understand software functionality. Our research aims to automatically analyze users interfaces to automate software tasks.</p> Open Science <p>Our lab follows open science practices. We strive to make all our papers, software, and data freely available to the public to support replication, scientific progress, and the advancement of software engineering research.</p>"}, {"location": "research/#research-sponsors", "title": "Research Sponsors", "text": "<p>The SAGE Lab graciously acknowledges support from our sponsors including the National Science Foundation, and Cisco Systems.</p> <p></p> Neural-based Code Search &amp; Refactoring <ul> <li>Award Info:  Cisco Advanced Security Research Grant</li> <li>Funding Amount:  $50,000</li> <li>Funding Period:  August, 2022 - August, 2023</li> </ul> Collaborative Research: CPS: Medium: Enabling Data-Driven Security and Safety Analyses for Cyber-Physical Systems <ul> <li>Award Info:  National Science Foundation Award CNS-2132285</li> <li>Funding Amount:  $998,979</li> <li>Funding Period:  October 1st, 2022 - September 30th, 2025</li> </ul> Automated Code Review for Measuring and Improving Software Maintainability <ul> <li>Award Info:  Cisco Advanced Security Research Grant</li> <li>Funding Amount:  $50,000</li> <li>Funding Period:  August, 2021 - August, 2022</li> </ul> <p></p> SHF: Small Towards a Holistic Causal Model for Continuous Software Traceability <ul> <li>Award Info:  National Science Foundation Award CCF-2007246</li> <li>Funding Amount:  $500,000</li> <li>Funding Period:  October 1st, 2020 - September 30th, 2023</li> </ul> Collaborative Research: SHF: Medium: Bug Report Management 2.0 <ul> <li>Award Info:  National Science Foundation Award CCF-1955853</li> <li>Funding Amount:  $1,200,000</li> <li>Funding Period:  October 1st, 2020 - September 30th, 2024</li> </ul> EAGER: Mapping Future Synergies between Deep Learning and Software Engineering <ul> <li>Award Info:  National Science Foundation Award CCF-1927679</li> <li>Funding Amount:  $82,828</li> <li>Funding Period:  July 15th, 2019 - June 30th, 2020</li> </ul> <p></p> Identifying and Tracing Security-Related Software Requirements <ul> <li>Award Info:  Cisco Advanced Security Research Grant</li> <li>Funding Amount:  $100,000</li> <li>Funding Period:  July, 2019 - July, 2020</li> </ul>"}, {"location": "team/", "title": "Team", "text": ""}, {"location": "team/#lab-director", "title": "Lab Director", "text": "Dr. Kevin Moran <ul> <li>Bio: Kevin Moran is an Assistant Professor in the Department of Computer Science at the University of Central Florida.  He graduated with his B.A. in Physics with a Computer Science Minor from the College of the Holy Cross in 2013. He graduated with his M.S. in Computer Science from William &amp; Mary in 2015, and his Ph.D. in Computer Science from William &amp; Mary in 2018. Dr. Moran directs the SAGE Research Lab.</li> <li> </li> <ul>"}, {"location": "team/#phd-students", "title": "Ph.D. Students", "text": "<ul> <li> <p> Sabiha Salma</p> <p></p> <ul> <li>4th Year Ph.D. Student in Computer Science at GMU</li> <li>Research Interests: Automated UI Analysis, HCI Considerations for Developer Tools, AI for Software Engineering</li> <li> Webpage</li> <li> itsmesabiha</li> </ul> </li> <li> <p> SM Hasan Mansur</p> <p></p> <ul> <li>4th Year Ph.D. Student in Computer Science at GMU</li> <li>Research Interests: Automated UI Analysis, AI for Software Engineering</li> <li> Webpage</li> </ul> </li> <li> <p> Junayed Mahmud</p> <p></p> <ul> <li>Ph.D. Student in Computer Science at UCF</li> <li>Research Interests: Bug Reporting, NLP for Software Engineering, Automated Mobile Testing</li> <li> Webpage</li> <li> JunayedMahmud10</li> </ul> </li> <li> <p> Safwat Ali Khan</p> <p></p> <ul> <li>Ph.D. Student in Computer Science at GMU</li> <li>Research Interests: Automated UI Analysis, Automated UI testing, Mobile Applications, Smart Home Testing</li> <li> LinkedIn</li> <li> safwatknopfler</li> </ul> </li> <li> <p> Arun Krishnavajjala</p> <p></p> <ul> <li>Ph.D. Student in Computer Science at GMU</li> <li>Research Interests: Software Engineering for Accessibility, Automated UI Analysis</li> <li> Webpage</li> <li> ItsArunKV</li> </ul> </li> <li> <p> Atish Dipongkor</p> <p></p> <ul> <li>Ph.D. Student in Computer Science at UCF</li> <li>Research Interests: NLP for Software Engineering, Bug Report Management,</li> <li> Webpage</li> <li> atish_iit</li> </ul> </li> </ul>"}, {"location": "team/#alumni", "title": "Alumni", "text": ""}, {"location": "team/#undergraduate-students", "title": "Undergraduate Students", "text": "<ul> <li> <p> Kristen Goebel</p> <p></p> <ul> <li>Undergraduate Student at Clarkson University</li> <li>Position after SAGE Lab: Graduate Student in Computer Science at Oregon State University</li> <li> LinkedIn</li> </ul> </li> <li> <p> Jasmine Obas</p> <p></p> <ul> <li>Undergraduate Student at George Mason University</li> <li> LinkedIn</li> </ul> </li> <li> <p> Yule Zhang</p> <p></p> <ul> <li>Undergraduate Student at George Mason University</li> <li>Position after SAGE Lab: Software Engineer at UPS</li> <li> LinkedIn</li> </ul> </li> </ul>"}, {"location": "team/#high-school-students", "title": "High School Students", "text": "<ul> <li> <p> Justin Jose</p> <p></p> <ul> <li>South Lakes High School</li> <li> LinkedIn</li> </ul> </li> <li> <p> Alyssa McGowan</p> <p></p> <ul> <li>Thomas Jefferson High School for Science and Technology</li> <li> LinkedIn</li> </ul> </li> <li> <p> Anish Pothireddy</p> <p></p> <ul> <li>Osbourn Park High School</li> <li>Position after SAGE Lab: Undergraduate at the University of Pennsylvania - Wharton School of Buisness</li> <li> LinkedIn</li> </ul> </li> <li> <p> Damilola Awofisayo</p> <p></p> <ul> <li>Thomas Jefferson High School for Science and Technology</li> <li>Position after SAGE Lab: Undergraduate Student in Computer Science at Duke University</li> <li> Webpage</li> </ul> </li> </ul>"}, {"location": "emulators/apod-1.6.2.3-cc3/", "title": "Apod 1.6.2.3 cc3", "text": ""}, {"location": "emulators/apod-1.6.2.3-cc3/#bug-id-apod-cc3", "title": "Bug ID: APOD-CC3", "text": ""}, {"location": "emulators/apod-1.6.2.3-cc3/#app-name-antennapod", "title": "App Name: AntennaPod", "text": ""}, {"location": "emulators/apod-1.6.2.3-cc3/#app-version-1623", "title": "App Version: 1.6.2.3", "text": "<p>please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/apod-1.6.2.3-rb/", "title": "Apod 1.6.2.3 rb", "text": ""}, {"location": "emulators/apod-1.6.2.3-rb/#bug-id-apod-rb", "title": "Bug ID: APOD-RB", "text": ""}, {"location": "emulators/apod-1.6.2.3-rb/#app-name-antennapod", "title": "App Name: AntennaPod", "text": ""}, {"location": "emulators/apod-1.6.2.3-rb/#app-version-1623", "title": "App Version: 1.6.2.3", "text": "<p>please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/droid-1.5.4-cc5/", "title": "Droid 1.5.4 cc5", "text": ""}, {"location": "emulators/droid-1.5.4-cc5/#bug-id-droid-cc5", "title": "Bug ID: Droid-CC5", "text": ""}, {"location": "emulators/droid-1.5.4-cc5/#app-name-droidweight", "title": "App Name: Droidweight", "text": ""}, {"location": "emulators/droid-1.5.4-cc5/#app-version-154", "title": "App Version: 1.5.4", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/droid-1.5.4-cc6/", "title": "Droid 1.5.4 cc6", "text": ""}, {"location": "emulators/droid-1.5.4-cc6/#bug-id-droid-cc6", "title": "Bug ID: Droid-CC6", "text": ""}, {"location": "emulators/droid-1.5.4-cc6/#app-name-droidweight", "title": "App Name: Droidweight", "text": ""}, {"location": "emulators/droid-1.5.4-cc6/#app-version-154", "title": "App Version: 1.5.4", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/gnucash-1.0.3-rc/", "title": "Gnucash 1.0.3 rc", "text": ""}, {"location": "emulators/gnucash-1.0.3-rc/#bug-id-gnu-rc", "title": "Bug ID: GNU-RC", "text": ""}, {"location": "emulators/gnucash-1.0.3-rc/#app-name-gnu-cash", "title": "App Name: GNU Cash", "text": ""}, {"location": "emulators/gnucash-1.0.3-rc/#app-version-103", "title": "App Version: 1.0.3", "text": "<p>please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/gnucash-2.1.3-cc9/", "title": "Gnucash 2.1.3 cc9", "text": ""}, {"location": "emulators/gnucash-2.1.3-cc9/#bug-id-gnu-cc9", "title": "Bug ID: GNU-CC9", "text": ""}, {"location": "emulators/gnucash-2.1.3-cc9/#app-name-gnu-cash", "title": "App Name: GNU Cash", "text": ""}, {"location": "emulators/gnucash-2.1.3-cc9/#app-version-213", "title": "App Version: 2.1.3", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/grow-2.3.1-cc5/", "title": "Grow 2.3.1 cc5", "text": ""}, {"location": "emulators/grow-2.3.1-cc5/#bug-id-grow-cc5", "title": "Bug ID: Grow-CC5", "text": ""}, {"location": "emulators/grow-2.3.1-cc5/#app-name-grow", "title": "App Name: Grow", "text": ""}, {"location": "emulators/grow-2.3.1-cc5/#app-version-231", "title": "App Version: 2.3.1", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/grow-2.3.1-rc/", "title": "Grow 2.3.1 rc", "text": ""}, {"location": "emulators/grow-2.3.1-rc/#bug-id-grow-rc", "title": "Bug ID: GROW-RC", "text": ""}, {"location": "emulators/grow-2.3.1-rc/#app-name-grow", "title": "App Name: Grow", "text": ""}, {"location": "emulators/grow-2.3.1-rc/#app-version-231", "title": "App Version: 2.3.1", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/mileage-3.1.1-53/", "title": "Mileage 3.1.1 53", "text": ""}, {"location": "emulators/mileage-3.1.1-53/#mileage-35", "title": "Mileage-35", "text": ""}, {"location": "emulators/mileage-3.1.1-53/#app-name-mileage", "title": "App Name: Mileage", "text": ""}, {"location": "emulators/mileage-3.1.1-53/#app-version-311", "title": "App Version: 3.1.1", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/time-0.20-cc1/", "title": "Time 0.20 cc1", "text": ""}, {"location": "emulators/time-0.20-cc1/#bug-id-time-cc1", "title": "Bug ID: TIME-CC1", "text": ""}, {"location": "emulators/time-0.20-cc1/#app-name-time-tracker", "title": "App Name: Time-Tracker", "text": ""}, {"location": "emulators/time-0.20-cc1/#app-version-020", "title": "App Version: 0.20", "text": "<p>please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/time-0.20-cc4/", "title": "Time 0.20 cc4", "text": ""}, {"location": "emulators/time-0.20-cc4/#bug-id-time-cc4", "title": "Bug ID: TIME-CC4", "text": ""}, {"location": "emulators/time-0.20-cc4/#app-name-time-tracker", "title": "App Name: Time Tracker", "text": ""}, {"location": "emulators/time-0.20-cc4/#app-version-020", "title": "App Version: 0.20", "text": "<p>please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/token-2.10-cc2/", "title": "Token 2.10 cc2", "text": ""}, {"location": "emulators/token-2.10-cc2/#bug-id-tok-cc2", "title": "Bug ID: TOK-CC2", "text": ""}, {"location": "emulators/token-2.10-cc2/#app-name-android-token", "title": "App Name: Android Token", "text": ""}, {"location": "emulators/token-2.10-cc2/#app-version-210", "title": "App Version: 2.10", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}, {"location": "emulators/token-2.10-cc7/", "title": "Token 2.10 cc7", "text": ""}, {"location": "emulators/token-2.10-cc7/#bug-id-tok-cc7", "title": "Bug ID: TOK-CC7", "text": ""}, {"location": "emulators/token-2.10-cc7/#app-name-android-token", "title": "App Name: Android Token", "text": ""}, {"location": "emulators/token-2.10-cc7/#app-version-210", "title": "App Version: 2.10", "text": "<p>Please use the emulator below to reproduce the bug.</p> <p> </p>"}]}